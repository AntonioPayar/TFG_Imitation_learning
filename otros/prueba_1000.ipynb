{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-11-01 22:19:23.712579: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-01 22:19:23.722565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-01 22:19:23.733117: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-01 22:19:23.736340: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-01 22:19:23.744683: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import (\n",
    "    layers,\n",
    "    models,\n",
    "    callbacks,\n",
    "    losses,\n",
    "    optimizers,\n",
    "    metrics,\n",
    ")\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "from PIL import Image, ImageEnhance ,ImageFilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU encontrada: /device:GPU:0\n",
      "Activada la asignacion de memoria gradual\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730495964.802737   68393 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730495964.831329   68393 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730495964.831487   68393 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730495964.874610   68393 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730495964.874751   68393 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730495964.874857   68393 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-01 22:19:24.874940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /device:GPU:0 with 6920 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "I0000 00:00:1730495964.875414   68393 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730495964.875535   68393 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730495964.875631   68393 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730495964.875739   68393 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730495964.875843   68393 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-01 22:19:24.875918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /device:GPU:0 with 6920 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "I0000 00:00:1730495964.876294   68393 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730495964.876413   68393 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730495964.876511   68393 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Verificar dispositivos disponibles\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU encontrada:', tf.test.gpu_device_name())\n",
    "else:\n",
    "    print(\"No se encontró GPU. Se utilizará la CPU.\")\n",
    "\n",
    "# Listar todas las GPU disponibles\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# Configurar el crecimiento de memoria para cada GPU\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        print(\"Activada la asignacion de memoria gradual\")\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo ya existe..\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    " # Comprobar si el archivo existe\n",
    "if not os.path.exists('../datos/grabacion/base_datos_cod_augmented.db'):\n",
    "    # Copiar el archivo\n",
    "    shutil.copy('../datos/grabacion/base_datos_cod.db', '../datos/grabacion/base_datos_cod_augmented.db')\n",
    "    print(\"Archivo copiado exitosamente..\")\n",
    "else:\n",
    "    print(\"Archivo ya existe..\")\n",
    "\n",
    "# Conectar a la base de datos (crea el archivo si no existe)\n",
    "conexion = sqlite3.connect(\"../datos/grabacion/base_datos_cod_augmented.db\")\n",
    "\n",
    "# Crear un cursor para interactuar con la base de datos\n",
    "cursor = conexion.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2939"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"SELECT MAX(id) FROM videos_mapa\")\n",
    "ID = cursor.fetchone()[0]\n",
    "\n",
    "if ID == None:\n",
    "    ID = 0\n",
    "\n",
    "ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANTIDAD_VIDEOS = 5\n",
    "\n",
    "cursor.execute(\"SELECT id as id_video, COUNT(*) AS cantidad FROM videos_mapa GROUP BY id HAVING COUNT(*) > \"+str(CANTIDAD_VIDEOS)+\"\")\n",
    "resultados = cursor.fetchall()\n",
    "\n",
    "# Obtener los nombres de las columnas\n",
    "columnas = [descripcion[0] for descripcion in cursor.description]\n",
    "\n",
    "# Crear el DataFrame\n",
    "df_ids = pd.DataFrame(resultados, columns=columnas)\n",
    "\n",
    "# Utilizando .to_numpy()\n",
    "ARRAY_ID = df_ids['id_video'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en entrenamiento y prueba (10% para prueba)\n",
    "X_train_Mapa, X_test_Mapa = train_test_split(ARRAY_ID, test_size=0.1, random_state=42)\n",
    "# Dividir entrenamiento en entrenamiento y validación (30% de 90% para validación)\n",
    "X_train_Mapa, X_valid_Mapa = train_test_split(X_train_Mapa, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ->1417\n",
      "valid ->608\n",
      "test ->225\n"
     ]
    }
   ],
   "source": [
    "print(\"train ->\"+str(len(X_train_Mapa)))\n",
    "print(\"valid ->\"+str(len(X_valid_Mapa)))\n",
    "print(\"test ->\"+str(len(X_test_Mapa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_dataframe(lista_ids):\n",
    "    # Crear un DataFrame vacío para almacenar los resultados\n",
    "    df_mapa = pd.DataFrame()\n",
    "    df_pov = pd.DataFrame()\n",
    "\n",
    "    for id in lista_ids:\n",
    "        query = f\"SELECT * FROM videos_mapa WHERE id == {id}\"\n",
    "        query_pov = f\"SELECT * FROM videos_pov WHERE id == {id}\"\n",
    "\n",
    "        # Leer el resultado de la consulta y añadirlo al DataFrame\n",
    "        df_temporal = pd.read_sql_query(query, conexion)\n",
    "        df_temporal_pov = pd.read_sql_query(query_pov, conexion)\n",
    "\n",
    "        # Concatenar al DataFrame principal\n",
    "        df_mapa = pd.concat([df_mapa, df_temporal], ignore_index=True)\n",
    "        df_pov = pd.concat([df_pov, df_temporal_pov], ignore_index=True)\n",
    "    \n",
    "    return df_mapa , df_pov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_mapa , df_train_pov = crear_dataframe(X_train_Mapa)\n",
    "df_valid_mapa , df_valid_pov = crear_dataframe(X_valid_Mapa)\n",
    "df_test_mapa , df_test_pov = crear_dataframe(X_test_Mapa)\n",
    "conexion.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def preprocesado_df(df):\n",
    "    # Convertir la columna de tipo object a listas de enteros\n",
    "    df['etiqueta'] = df['etiqueta'].apply(ast.literal_eval)\n",
    "    # Eliminamos las filas donde 'mouse_final' sea igual a [0, 4]\n",
    "    filtro = df['etiqueta'].apply(lambda x: x[1] == 4)\n",
    "    df = df[~filtro]\n",
    "    # Crear un filtro booleano para las filas que cumplen la condición\n",
    "    condicion = df['etiqueta'].apply(lambda x: x[1] == 2)\n",
    "    # Juntamos las pulsaciones y tecla en una columna\n",
    "    #x_positions_2 = df['etiqueta'].apply(lambda x: str(x[0])+str(x[1]))\n",
    "    x_positions_2 = df['etiqueta'].apply(lambda x: str(x[1]))\n",
    "    df['etiqueta'] = x_positions_2\n",
    "    # Modificar solo las filas que cumplen con la condición\n",
    "    #df.loc[condicion, 'etiqueta'] = df.loc[condicion, 'etiqueta'].apply(lambda x: \"12\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_mapa = preprocesado_df(df_train_mapa)\n",
    "df_valid_mapa = preprocesado_df(df_valid_mapa)\n",
    "df_test_mapa = preprocesado_df(df_test_mapa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extraer la primera posición sin modificar el DataFrame\n",
    "x_positions = df_train_mapa['etiqueta'].apply(lambda x: x[0])\n",
    "\n",
    "# Crear la figura y el eje\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Crear el histograma\n",
    "ax.hist(x_positions, bins=10, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Agregar el título y las etiquetas de los ejes\n",
    "ax.set_title('Distribución de la posición x')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Frecuencia')\n",
    "\n",
    "# Mostrar la figura\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "def aplicar_one_hot_balanceo(df,columa):\n",
    "    # Crear un DataFrame con las columnas de one-hot encoding\n",
    "    one_hot_df = pd.get_dummies(df[columa], prefix='label')\n",
    "\n",
    "    # Unir el DataFrame original con el DataFrame de one-hot encoding\n",
    "    df = pd.concat([df, one_hot_df], axis=1)\n",
    "\n",
    "    # Balanceamos los pesos para el entrenamiento\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(df[columa]),\n",
    "        y=df[columa]\n",
    "    )\n",
    "\n",
    "    # Crear un diccionario de pesos para las clases\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "    #Eliminamos la columna original\n",
    "    df = df.drop(columa, axis=1)\n",
    "    return df , class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_mapa , pesos_train = aplicar_one_hot_balanceo(df_train_mapa,\"etiqueta\")\n",
    "df_valid_mapa , pesos_valid = aplicar_one_hot_balanceo(df_valid_mapa,\"etiqueta\")\n",
    "df_test_mapa , pesos_test= aplicar_one_hot_balanceo(df_test_mapa,\"etiqueta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETIQUETAS = np.array(df_train_mapa.filter(like='label').columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "class ImageDataGenerator(Sequence):\n",
    "    def __init__(self, df , sequence_length ,image_height , image_width , batch_size, filtro, **kwargs):\n",
    "        self.df = df\n",
    "        self.image_sequence_length = sequence_length\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.batch_size = batch_size\n",
    "        self.POV = filtro\n",
    "        super().__init__(**kwargs)  # Llamada al constructor de la clase base\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / float(self.batch_size)))\n",
    "\n",
    "    def cargar_todo(self,fin_df):\n",
    "        batch_df = self.df.iloc[0:fin_df]     #Obtenemos un lote del df\n",
    "\n",
    "        X_batch = np.zeros((len(batch_df), self.image_sequence_length ,self.image_height, self.image_width, 3))  # Inicializar matriz para las imágenes\n",
    "        #y_batch = batch_df[['izquieda', 'derecha', 'delante']].values\n",
    "        y_batch = np.array(batch_df['mouse_final'].tolist())\n",
    "\n",
    "        for i, (_, row) in enumerate(batch_df.iterrows()):\n",
    "            frames = []\n",
    "            for j in range(self.image_sequence_length):\n",
    "                imagen = Image.open(\"../\" + row.iloc[j])    # Obtener la ruta de la imagen\n",
    "                imagen = Image.fromarray(np.array(imagen)).filter(ImageFilter.SHARPEN)  #Aplicamos Filtro SHARPEN\n",
    "                imagen = np.array(imagen) / 255.0           # Normalizar la imagen\n",
    "\n",
    "                frames.append(imagen)                  #Juntamos frames en lista\n",
    "            video = np.stack(frames, axis=0)           # Convertir lista de frames en un video np\n",
    "            X_batch[i] = video\n",
    "\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inicio_lote = index * self.batch_size\n",
    "        final_lote = (index + 1) * self.batch_size\n",
    "\n",
    "        batch_df = self.df.iloc[inicio_lote:final_lote]     #Obtenemos un lote del df\n",
    "\n",
    "        X_batch = np.zeros((len(batch_df), self.image_sequence_length ,self.image_height, self.image_width, 3))  # Inicializar matriz para las imágenes\n",
    "        # Obtener y_batch después del one-hot encoding\n",
    "        #y_batch = batch_df[['izquieda', 'derecha','delante']].values\n",
    "        #y_batch = np.array(batch_df['mouse_final'].tolist())\n",
    "\n",
    "        # Filtrar las columnas que comienzan con 'label'\n",
    "        y_batch = batch_df.filter(like='label').values\n",
    "\n",
    "        for i, (_, row) in enumerate(batch_df.iterrows()):\n",
    "            frames = []\n",
    "            for j in range(self.image_sequence_length):\n",
    "                imagen = Image.open(\"../\" + row.iloc[j+2])    # Obtener la ruta de la imagen\n",
    "                ancho, alto = imagen.size\n",
    "\n",
    "                if self.POV == True :\n",
    "                    imagen = imagen.rotate(2.5, expand=True)\n",
    "                    # Recorta la imagen (izquierda, superior, derecha, inferior)\n",
    "                    imagen = imagen.crop((5, 12, ancho - 35, alto-25))\n",
    "                    imagen = ImageEnhance.Sharpness(imagen)\n",
    "                    imagen = imagen.enhance(7)  # Aumento de contraste\n",
    "\n",
    "                imagen = np.array(imagen) / 255.0           # Normalizar la imagen\n",
    "\n",
    "                frames.append(imagen)                  #Juntamos frames en lista\n",
    "            video = np.stack(frames, axis=0)           # Convertir lista de frames en un video np\n",
    "            X_batch[i] = video\n",
    "\n",
    "        return X_batch, y_batch\n",
    "\n",
    "def comprobar_contenido(X_train , Y_train , num_video):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(5 * 2, 1 * 2))\n",
    "\n",
    "    for i in range(5):\n",
    "        axs[i].imshow(X_train[num_video][i])\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(Y_train[num_video])\n",
    "    return Y_train[num_video]\n",
    "\n",
    "\n",
    "class ImageDataGeneratorVAR(ImageDataGenerator):\n",
    "    def __init__(self, df , sequence_length ,image_height , image_width , batch_size, filtro,autoencoder,encoder = None,**kwargs):\n",
    "        self.autoencoder = autoencoder\n",
    "        self.encoder_entrenado = encoder\n",
    "        super().__init__(df, sequence_length, image_height, image_width, batch_size, filtro, **kwargs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / float(self.batch_size)))\n",
    "\n",
    "    def cargar_todo(self,fin_df):\n",
    "        batch_df = self.df.iloc[0:fin_df]     #Obtenemos un lote del df\n",
    "\n",
    "        X_batch = np.zeros((len(batch_df), self.image_sequence_length ,self.image_height, self.image_width, 3))  # Inicializar matriz para las imágenes\n",
    "        #y_batch = batch_df[['izquieda', 'derecha', 'delante']].values\n",
    "        y_batch = np.array(batch_df['mouse_final'].tolist())\n",
    "\n",
    "        for i, (_, row) in enumerate(batch_df.iterrows()):\n",
    "            frames = []\n",
    "            for j in range(self.image_sequence_length):\n",
    "                imagen = Image.open(\"../\" + row.iloc[j])    # Obtener la ruta de la imagen\n",
    "                imagen = Image.fromarray(np.array(imagen)).filter(ImageFilter.SHARPEN)  #Aplicamos Filtro SHARPEN\n",
    "                imagen = np.array(imagen) / 255.0           # Normalizar la imagen\n",
    "\n",
    "                frames.append(imagen)                  #Juntamos frames en lista\n",
    "            video = np.stack(frames, axis=0)           # Convertir lista de frames en un video np\n",
    "            X_batch[i] = video\n",
    "\n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def __concatenar_imgs_individual__(self,X_batch):\n",
    "        batch_independiente = []\n",
    "        for i in range(X_batch.shape[0]):\n",
    "            batch_independiente.append(X_batch[i][4])\n",
    "        return np.array(batch_independiente)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inicio_lote = index * self.batch_size\n",
    "        final_lote = (index + 1) * self.batch_size\n",
    "\n",
    "        batch_df = self.df.iloc[inicio_lote:final_lote]     #Obtenemos un lote del df\n",
    "\n",
    "        # Inicializar matriz para las imágenes\n",
    "        #X_batch = np.zeros((len(batch_df), self.image_sequence_length ,self.image_height, self.image_width, 3))\n",
    "        X_batch = np.zeros((len(batch_df), self.image_sequence_length, 120, 120, 3))  # Ajustar a 100x100\n",
    "        \n",
    "        # Filtrar las columnas que comienzan con 'label'\n",
    "        y_batch = batch_df.filter(like='label').values\n",
    "\n",
    "        for i, (_, row) in enumerate(batch_df.iterrows()):\n",
    "            frames = []\n",
    "            for j in range(self.image_sequence_length):\n",
    "                imagen = Image.open(\"../\" + row.iloc[j+2])    # Obtener la ruta de la imagen\n",
    "                ancho, alto = imagen.size\n",
    "\n",
    "                if self.POV == True :\n",
    "                    imagen = imagen.rotate(2.5, expand=True)\n",
    "                    # Recorta la imagen (izquierda, superior, derecha, inferior)\n",
    "                    imagen = imagen.crop((5, 12, ancho - 35, alto-25))\n",
    "                    imagen = ImageEnhance.Sharpness(imagen)\n",
    "                    imagen = imagen.enhance(7)  # Aumento de contraste\n",
    "\n",
    "                imagen = imagen.resize((120, 120))          #(100, 100)\n",
    "                imagen = np.array(imagen).astype(\"float32\")  / 255.0           # Normalizar la imagen\n",
    "\n",
    "                frames.append(imagen)                  #Juntamos frames en lista\n",
    "            video = np.stack(frames, axis=0)           # Convertir lista de frames en un video np\n",
    "            X_batch[i] = video\n",
    "        \n",
    "        #Para autoencoder\n",
    "        if self.autoencoder == True:\n",
    "            return self.__concatenar_imgs_individual__(X_batch)\n",
    "        else : \n",
    "            return self.__concatenar_imgs_individual__(X_batch), y_batch\n",
    "\n",
    "def comprobar_contenido(X_train , num_video):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(5 * 2, 1 * 2))\n",
    "\n",
    "    for i in range(5):\n",
    "        axs[i].imshow(X_train[num_video][i])\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_entrenado = None\n",
    "autoencoder = True\n",
    "# Generador Mapa\n",
    "altura_img_mini = 188\n",
    "anchura_img_mini = 260\n",
    "batch_size = BATCH \n",
    "\n",
    "# Generadores para autoencoder\n",
    "train_generator_Mapa = ImageDataGeneratorVAR(df_train_mapa, sequence_length=5, image_height=altura_img_mini, image_width=anchura_img_mini, batch_size=batch_size ,filtro =True , autoencoder = autoencoder , encoder= encoder_entrenado)\n",
    "test_generator_Mapa = ImageDataGeneratorVAR(df_test_mapa, sequence_length=5, image_height=altura_img_mini, image_width=anchura_img_mini, batch_size=batch_size ,filtro =True, autoencoder = autoencoder, encoder= encoder_entrenado)\n",
    "valid_generator_Mapa = ImageDataGeneratorVAR(df_valid_mapa, sequence_length=5, image_height=altura_img_mini, image_width=anchura_img_mini, batch_size=batch_size ,filtro =True, autoencoder = autoencoder, encoder= encoder_entrenado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(models.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Call the model on a particular input.\"\"\"\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return z_mean, z_log_var, reconstruction\n",
    "\n",
    "    def train_step(self, train_generator):\n",
    "        \"\"\"Step run during training.\"\"\"\n",
    "        for step in range(len(train_generator)):\n",
    "            data = train_generator[step]\n",
    "            with tf.GradientTape() as tape:\n",
    "                #TensorFlow's Gradient Tape helps calculate gradients during a forward pass.\n",
    "                #To use it, we need wrap the code that performs the operations you want to differentiate within a tf.GradientTape() context.\n",
    "                #After recording the operations, we can compute the gradient of the loss function concerning certain variables using tape.gradient().\n",
    "                #These gradients are then used to update the variables with the optimizer.\n",
    "\n",
    "                z_mean, z_log_var, reconstruction = self(data)\n",
    "                beta = 500\n",
    "                reconstruction_loss = tf.reduce_mean(\n",
    "                    beta\n",
    "                    * losses.binary_crossentropy(\n",
    "                        data, reconstruction, axis=(1, 2, 3)\n",
    "                    )\n",
    "                )\n",
    "                kl_loss = tf.reduce_mean(\n",
    "                    tf.reduce_sum(\n",
    "                        -0.5\n",
    "                        * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n",
    "                        axis=1,\n",
    "                    )\n",
    "                )\n",
    "                total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        \"\"\"Step run during validation.\"\"\"\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        z_mean, z_log_var, reconstruction = self(data)\n",
    "        beta = 500\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            beta\n",
    "            * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3))\n",
    "        )\n",
    "        kl_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n",
    "                axis=1,\n",
    "            )\n",
    "        )\n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten ,Conv2DTranspose,Reshape,Dense,BatchNormalization,Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "#Sampling Layer\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        # Cambiar K.random_normal a tf.random.normal\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # La salida de la capa tiene la misma forma que z_mean y z_log_var\n",
    "        return input_shape[0]\n",
    "\n",
    "def get_encoder_primero(input_shape,embed_dim):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same',kernel_regularizer=l2(0.01))(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same',kernel_regularizer=l2(0.01))(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same',kernel_regularizer=l2(0.01))(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    encoded = Flatten()(x)\n",
    "\n",
    "    z_mean = layers.Dense(embed_dim, name=\"z_mean\")(encoded)\n",
    "    z_log_var = layers.Dense(embed_dim, name=\"z_log_var\")(encoded)\n",
    "\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    Encoder_primero = tf.keras.Model(inputs=inputs,outputs=[z_mean, z_log_var, z])\n",
    "    return Encoder_primero ,encoded.shape\n",
    "\n",
    "def get_decoder(input_shape):\n",
    "    z = Input(shape=input_shape)\n",
    "    x = Dense(28800 ,activation=\"relu\",kernel_regularizer=l2(0.01))(z) \n",
    "    x = Reshape((15, 15, 128))(x)\n",
    "    x = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\",kernel_regularizer=l2(0.01))(x)\n",
    "    x = Conv2DTranspose(32, (3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\",kernel_regularizer=l2(0.01))(x)\n",
    "    x = Conv2DTranspose(16, (3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\",kernel_regularizer=l2(0.01))(x)\n",
    "    decoded_img = Conv2DTranspose(3, (2, 2), padding=\"same\", activation=\"sigmoid\")(x)\n",
    "\n",
    "    Decoder = tf.keras.Model(inputs=z,outputs=[decoded_img])\n",
    "    return Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESPACIO_LATENTE = 10\n",
    "height = 120\n",
    "width = 120\n",
    "channels = 3\n",
    "input_shape_pov = (height, width, channels)\n",
    "\n",
    "encoder,flatten_layer = get_encoder_primero(input_shape_pov,ESPACIO_LATENTE)\n",
    "dimension = []\n",
    "dimension.append(ESPACIO_LATENTE + 0)\n",
    "size_flatten_etiqueta = tuple(dimension)\n",
    "decoder = get_decoder(size_flatten_etiqueta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_6     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_7     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ max_pooling2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_8     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28800</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_mean (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">288,010</span> │ flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_log_var (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">288,010</span> │ flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sampling_2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ z_mean[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sampling</span>)          │                   │            │ z_log_var[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m120\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m120\u001b[0m,  │        \u001b[38;5;34m896\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_6     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_6[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_7     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m,    │     \u001b[38;5;34m73,856\u001b[0m │ max_pooling2d_7[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_8     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28800\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_8[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_mean (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │    \u001b[38;5;34m288,010\u001b[0m │ flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_log_var (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │    \u001b[38;5;34m288,010\u001b[0m │ flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sampling_2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ z_mean[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mSampling\u001b[0m)          │                   │            │ z_log_var[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">669,268</span> (2.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m669,268\u001b[0m (2.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">669,268</span> (2.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m669,268\u001b[0m (2.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28800</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">316,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_8              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,792</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_9              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_10             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,624</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_11             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28800\u001b[0m)          │       \u001b[38;5;34m316,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_2 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_8              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m73,792\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_9              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m18,464\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_10             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │         \u001b[38;5;34m4,624\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_11             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │           \u001b[38;5;34m195\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">413,875</span> (1.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m413,875\u001b[0m (1.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">413,875</span> (1.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m413,875\u001b[0m (1.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder.summary()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener un lote de prueba (imágenes y etiquetas)\n",
    "X_batch = test_generator_Mapa.__getitem__(0)  # Obtenemos un lote de test\n",
    "\n",
    "for i in range(0,batch_size):\n",
    "    plt.imshow(X_batch[i])\n",
    "    plt.axis('off')  # Opcional: oculta los ejes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 22:38:29.438968: W tensorflow/core/framework/op_kernel.cc:1828] INVALID_ARGUMENT: TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float64,), but the yielded element was [[[[0.20392157 0.17254902 0.18431373]\n",
      "   [0.24705882 0.23529412 0.23921569]\n",
      "   [0.20392157 0.20784314 0.20392157]\n",
      "   ...\n",
      "   [0.99607843 0.98823529 0.98039216]\n",
      "   [0.98823529 0.98039216 0.97254902]\n",
      "   [0.96862745 0.95294118 0.93725491]]\n",
      "\n",
      "  [[0.38431373 0.32549021 0.34509805]\n",
      "   [0.34509805 0.33725491 0.33725491]\n",
      "   [0.34117648 0.35686275 0.34901962]\n",
      "   ...\n",
      "   [0.78823531 0.78039217 0.7764706 ]\n",
      "   [0.94117647 0.94117647 0.93725491]\n",
      "   [0.98039216 0.97647059 0.96862745]]\n",
      "\n",
      "  [[0.3137255  0.25490198 0.27450982]\n",
      "   [0.15686275 0.15686275 0.15686275]\n",
      "   [0.26666668 0.28235295 0.27058825]\n",
      "   ...\n",
      "   [0.20392157 0.18431373 0.20392157]\n",
      "   [0.70980394 0.7019608  0.70980394]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.31764707 0.34509805 0.42352942]\n",
      "   [0.23529412 0.25882354 0.33725491]\n",
      "   ...\n",
      "   [0.31764707 0.31764707 0.35686275]\n",
      "   [0.27450982 0.29411766 0.30980393]\n",
      "   [0.3137255  0.32941177 0.36470589]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.34509805 0.37254903 0.42745098]\n",
      "   [0.25490198 0.28235295 0.32549021]\n",
      "   ...\n",
      "   [0.31764707 0.32941177 0.35294119]\n",
      "   [0.29803923 0.3137255  0.33725491]\n",
      "   [0.30588236 0.30980393 0.34117648]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.25490198 0.27843139 0.3137255 ]\n",
      "   [0.29411766 0.32156864 0.36470589]\n",
      "   ...\n",
      "   [0.32156864 0.33333334 0.36470589]\n",
      "   [0.29019609 0.30980393 0.35294119]\n",
      "   [0.28627452 0.3019608  0.34509805]]]\n",
      "\n",
      "\n",
      " [[[0.10980392 0.13725491 0.18039216]\n",
      "   [0.15294118 0.18431373 0.23529412]\n",
      "   [0.15686275 0.16470589 0.20784314]\n",
      "   ...\n",
      "   [0.96862745 0.97647059 0.98823529]\n",
      "   [0.90980393 0.9137255  0.92941177]\n",
      "   [0.84705883 0.8509804  0.87450981]]\n",
      "\n",
      "  [[0.2        0.24705882 0.32941177]\n",
      "   [0.23529412 0.28235295 0.36470589]\n",
      "   [0.23921569 0.25490198 0.31764707]\n",
      "   ...\n",
      "   [0.78431374 0.7764706  0.78823531]\n",
      "   [0.94901961 0.94901961 0.95294118]\n",
      "   [0.90980393 0.90588236 0.92156863]]\n",
      "\n",
      "  [[0.15294118 0.18431373 0.24705882]\n",
      "   [0.14901961 0.17254902 0.23529412]\n",
      "   [0.16078432 0.16078432 0.20784314]\n",
      "   ...\n",
      "   [0.13725491 0.10980392 0.13333334]\n",
      "   [0.66274512 0.65098041 0.66666669]\n",
      "   [0.94117647 0.94509804 0.95686275]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.34901962 0.43529412 0.50588238]\n",
      "   [0.32549021 0.38039216 0.45882353]\n",
      "   ...\n",
      "   [0.38039216 0.36078432 0.43137255]\n",
      "   [0.33725491 0.3019608  0.3882353 ]\n",
      "   [0.38039216 0.32941177 0.42352942]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.38039216 0.44705883 0.52156866]\n",
      "   [0.34509805 0.41176471 0.48235294]\n",
      "   ...\n",
      "   [0.38039216 0.33333334 0.42352942]\n",
      "   [0.39607844 0.33725491 0.43137255]\n",
      "   [0.39607844 0.33725491 0.42745098]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.25882354 0.29803923 0.34509805]\n",
      "   [0.32549021 0.38431373 0.44705883]\n",
      "   ...\n",
      "   [0.40392157 0.34117648 0.44313726]\n",
      "   [0.39215687 0.32941177 0.42352942]\n",
      "   [0.3882353  0.32941177 0.41960785]]]\n",
      "\n",
      "\n",
      " [[[0.23137255 0.28235295 0.3019608 ]\n",
      "   [0.3137255  0.36862746 0.40000001]\n",
      "   [0.31764707 0.36470589 0.40000001]\n",
      "   ...\n",
      "   [0.47450981 0.59607846 0.70588237]\n",
      "   [0.36862746 0.49019608 0.60784316]\n",
      "   [0.25490198 0.38039216 0.49019608]]\n",
      "\n",
      "  [[0.41568628 0.50588238 0.56470591]\n",
      "   [0.48627451 0.57254905 0.64313728]\n",
      "   [0.48235294 0.56078434 0.63137257]\n",
      "   ...\n",
      "   [0.40000001 0.48235294 0.59607846]\n",
      "   [0.45882353 0.54901963 0.66666669]\n",
      "   [0.32941177 0.43529412 0.53725493]]\n",
      "\n",
      "  [[0.32156864 0.38039216 0.43921569]\n",
      "   [0.3019608  0.36470589 0.42352942]\n",
      "   [0.29803923 0.35294119 0.41568628]\n",
      "   ...\n",
      "   [0.00784314 0.03137255 0.14509805]\n",
      "   [0.24313726 0.29803923 0.42352942]\n",
      "   [0.3764706  0.46666667 0.58039218]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.49019608 0.35686275 0.48235294]\n",
      "   [0.3882353  0.27450982 0.3764706 ]\n",
      "   ...\n",
      "   [0.11764706 0.19607843 0.24313726]\n",
      "   [0.07450981 0.14509805 0.19215687]\n",
      "   [0.10588235 0.16470589 0.22352941]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.50588238 0.38039216 0.49803922]\n",
      "   [0.36862746 0.25098041 0.36078432]\n",
      "   ...\n",
      "   [0.09411765 0.15686275 0.21960784]\n",
      "   [0.10196079 0.16078432 0.22745098]\n",
      "   [0.10588235 0.16862746 0.22352941]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.36078432 0.29411766 0.36470589]\n",
      "   [0.43921569 0.33725491 0.44313726]\n",
      "   ...\n",
      "   [0.10980392 0.17647059 0.23921569]\n",
      "   [0.10980392 0.17647059 0.23921569]\n",
      "   [0.10980392 0.17647059 0.23921569]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.21568628 0.17254902 0.2       ]\n",
      "   [0.32156864 0.23529412 0.24313726]\n",
      "   [0.36862746 0.22352941 0.24705882]\n",
      "   ...\n",
      "   [0.85882354 0.78823531 0.8392157 ]\n",
      "   [0.79215688 0.69803923 0.73725492]\n",
      "   [0.6901961  0.57647061 0.6156863 ]]\n",
      "\n",
      "  [[0.39215687 0.32549021 0.36862746]\n",
      "   [0.47450981 0.35294119 0.34901962]\n",
      "   [0.65882355 0.35686275 0.40000001]\n",
      "   ...\n",
      "   [0.64705884 0.63529414 0.71372551]\n",
      "   [0.80784315 0.76078433 0.81960785]\n",
      "   [0.73333335 0.63921571 0.68235296]]\n",
      "\n",
      "  [[0.29411766 0.24705882 0.27843139]\n",
      "   [0.28235295 0.21960784 0.20784314]\n",
      "   [0.47450981 0.23137255 0.25098041]\n",
      "   ...\n",
      "   [0.05882353 0.09803922 0.23137255]\n",
      "   [0.45490196 0.43921569 0.53333336]\n",
      "   [0.73333335 0.66666669 0.73725492]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.36470589 0.27843139 0.47058824]\n",
      "   [0.33725491 0.23137255 0.41568628]\n",
      "   ...\n",
      "   [0.00392157 0.3137255  0.65490198]\n",
      "   [0.         0.26666668 0.59607846]\n",
      "   [0.         0.30588236 0.64705884]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.35294119 0.30980393 0.51372552]\n",
      "   [0.30588236 0.23529412 0.45490196]\n",
      "   ...\n",
      "   [0.00392157 0.26274511 0.6156863 ]\n",
      "   [0.01960784 0.25882354 0.61176473]\n",
      "   [0.00784314 0.27450982 0.63137257]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.24705882 0.28235295 0.39215687]\n",
      "   [0.30980393 0.35294119 0.52941179]\n",
      "   ...\n",
      "   [0.02745098 0.25882354 0.61960787]\n",
      "   [0.03921569 0.24705882 0.60000002]\n",
      "   [0.03137255 0.25490198 0.60784316]]]\n",
      "\n",
      "\n",
      " [[[0.30588236 0.29803923 0.30980393]\n",
      "   [0.40000001 0.40000001 0.39607844]\n",
      "   [0.40000001 0.40000001 0.40000001]\n",
      "   ...\n",
      "   [0.81568629 0.98039216 1.        ]\n",
      "   [0.73333335 0.92941177 1.        ]\n",
      "   [0.60784316 0.8509804  0.98823529]]\n",
      "\n",
      "  [[0.58039218 0.5411765  0.60784316]\n",
      "   [0.64313728 0.63137257 0.68627453]\n",
      "   [0.59215689 0.58823532 0.627451  ]\n",
      "   ...\n",
      "   [0.64313728 0.80784315 0.94509804]\n",
      "   [0.78431374 0.9254902  0.98039216]\n",
      "   [0.67843139 0.89019608 0.99215686]]\n",
      "\n",
      "  [[0.4509804  0.39215687 0.47058824]\n",
      "   [0.44705883 0.40000001 0.47843137]\n",
      "   [0.42745098 0.38431373 0.44313726]\n",
      "   ...\n",
      "   [0.03137255 0.20392157 0.3764706 ]\n",
      "   [0.45882353 0.63529414 0.7764706 ]\n",
      "   [0.74901962 0.9254902  1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.1882353  0.26274511 0.43529412]\n",
      "   [0.16862746 0.22745098 0.38431373]\n",
      "   ...\n",
      "   [0.16078432 0.17647059 0.10980392]\n",
      "   [0.1254902  0.12941177 0.06666667]\n",
      "   [0.15294118 0.15686275 0.09411765]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.21176471 0.32549021 0.49803922]\n",
      "   [0.13725491 0.24313726 0.40000001]\n",
      "   ...\n",
      "   [0.16470589 0.17254902 0.12941177]\n",
      "   [0.17647059 0.17254902 0.1254902 ]\n",
      "   [0.18039216 0.16862746 0.12156863]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.15686275 0.26666668 0.36862746]\n",
      "   [0.18039216 0.32549021 0.4627451 ]\n",
      "   ...\n",
      "   [0.19215687 0.18431373 0.12941177]\n",
      "   [0.18039216 0.16470589 0.1254902 ]\n",
      "   [0.18039216 0.16470589 0.12156863]]]\n",
      "\n",
      "\n",
      " [[[0.21568628 0.22352941 0.30980393]\n",
      "   [0.30588236 0.35294119 0.39607844]\n",
      "   [0.28235295 0.38431373 0.40000001]\n",
      "   ...\n",
      "   [0.66274512 0.67450982 0.64705884]\n",
      "   [0.58039218 0.58039218 0.53725493]\n",
      "   [0.47450981 0.47843137 0.41960785]]\n",
      "\n",
      "  [[0.42745098 0.43529412 0.62352943]\n",
      "   [0.47058824 0.54901963 0.67843139]\n",
      "   [0.40000001 0.56862748 0.63529414]\n",
      "   ...\n",
      "   [0.52549022 0.57647061 0.57647061]\n",
      "   [0.66666669 0.6901961  0.66274512]\n",
      "   [0.54509807 0.54901963 0.50588238]]\n",
      "\n",
      "  [[0.39607844 0.41960785 0.57254905]\n",
      "   [0.3137255  0.38431373 0.50980395]\n",
      "   [0.27450982 0.38431373 0.4627451 ]\n",
      "   ...\n",
      "   [0.01960784 0.08627451 0.11764706]\n",
      "   [0.32549021 0.36078432 0.35294119]\n",
      "   [0.53725493 0.5411765  0.50588238]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.22745098 0.29411766 0.27843139]\n",
      "   [0.23529412 0.26274511 0.25098041]\n",
      "   ...\n",
      "   [0.12941177 0.27058825 0.20784314]\n",
      "   [0.07843138 0.21960784 0.16078432]\n",
      "   [0.11764706 0.23921569 0.20392157]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.21176471 0.34901962 0.34901962]\n",
      "   [0.16470589 0.28627452 0.29411766]\n",
      "   ...\n",
      "   [0.18039216 0.24705882 0.21568628]\n",
      "   [0.2        0.25098041 0.23529412]\n",
      "   [0.19607843 0.25490198 0.21960784]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.18039216 0.29019609 0.29803923]\n",
      "   [0.23137255 0.38039216 0.38039216]\n",
      "   ...\n",
      "   [0.22352941 0.26274511 0.23921569]\n",
      "   [0.22745098 0.25098041 0.23921569]\n",
      "   [0.22745098 0.25882354 0.23529412]]]].\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 204, in generator_py_func\n",
      "    flattened_values = nest.flatten_up_to(output_types, values)\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/data/util/nest.py\", line 237, in flatten_up_to\n",
      "    return nest_util.flatten_up_to(\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/util/nest_util.py\", line 1541, in flatten_up_to\n",
      "    return _tf_data_flatten_up_to(shallow_tree, input_tree)\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/util/nest_util.py\", line 1570, in _tf_data_flatten_up_to\n",
      "    _tf_data_assert_shallow_structure(shallow_tree, input_tree)\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/util/nest_util.py\", line 1414, in _tf_data_assert_shallow_structure\n",
      "    raise TypeError(\n",
      "\n",
      "TypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: 'ndarray'.\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 206, in generator_py_func\n",
      "    raise TypeError(\n",
      "\n",
      "TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float64,), but the yielded element was [[[[0.20392157 0.17254902 0.18431373]\n",
      "   [0.24705882 0.23529412 0.23921569]\n",
      "   [0.20392157 0.20784314 0.20392157]\n",
      "   ...\n",
      "   [0.99607843 0.98823529 0.98039216]\n",
      "   [0.98823529 0.98039216 0.97254902]\n",
      "   [0.96862745 0.95294118 0.93725491]]\n",
      "\n",
      "  [[0.38431373 0.32549021 0.34509805]\n",
      "   [0.34509805 0.33725491 0.33725491]\n",
      "   [0.34117648 0.35686275 0.34901962]\n",
      "   ...\n",
      "   [0.78823531 0.78039217 0.7764706 ]\n",
      "   [0.94117647 0.94117647 0.93725491]\n",
      "   [0.98039216 0.97647059 0.96862745]]\n",
      "\n",
      "  [[0.3137255  0.25490198 0.27450982]\n",
      "   [0.15686275 0.15686275 0.15686275]\n",
      "   [0.26666668 0.28235295 0.27058825]\n",
      "   ...\n",
      "   [0.20392157 0.18431373 0.20392157]\n",
      "   [0.70980394 0.7019608  0.70980394]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.31764707 0.34509805 0.42352942]\n",
      "   [0.23529412 0.25882354 0.33725491]\n",
      "   ...\n",
      "   [0.31764707 0.31764707 0.35686275]\n",
      "   [0.27450982 0.29411766 0.30980393]\n",
      "   [0.3137255  0.32941177 0.36470589]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.34509805 0.37254903 0.42745098]\n",
      "   [0.25490198 0.28235295 0.32549021]\n",
      "   ...\n",
      "   [0.31764707 0.32941177 0.35294119]\n",
      "   [0.29803923 0.3137255  0.33725491]\n",
      "   [0.30588236 0.30980393 0.34117648]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.25490198 0.27843139 0.3137255 ]\n",
      "   [0.29411766 0.32156864 0.36470589]\n",
      "   ...\n",
      "   [0.32156864 0.33333334 0.36470589]\n",
      "   [0.29019609 0.30980393 0.35294119]\n",
      "   [0.28627452 0.3019608  0.34509805]]]\n",
      "\n",
      "\n",
      " [[[0.10980392 0.13725491 0.18039216]\n",
      "   [0.15294118 0.18431373 0.23529412]\n",
      "   [0.15686275 0.16470589 0.2"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float64,), but the yielded element was [[[[0.20392157 0.17254902 0.18431373]\n   [0.24705882 0.23529412 0.23921569]\n   [0.20392157 0.20784314 0.20392157]\n   ...\n   [0.99607843 0.98823529 0.98039216]\n   [0.98823529 0.98039216 0.97254902]\n   [0.96862745 0.95294118 0.93725491]]\n\n  [[0.38431373 0.32549021 0.34509805]\n   [0.34509805 0.33725491 0.33725491]\n   [0.34117648 0.35686275 0.34901962]\n   ...\n   [0.78823531 0.78039217 0.7764706 ]\n   [0.94117647 0.94117647 0.93725491]\n   [0.98039216 0.97647059 0.96862745]]\n\n  [[0.3137255  0.25490198 0.27450982]\n   [0.15686275 0.15686275 0.15686275]\n   [0.26666668 0.28235295 0.27058825]\n   ...\n   [0.20392157 0.18431373 0.20392157]\n   [0.70980394 0.7019608  0.70980394]\n   [1.         1.         1.        ]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.31764707 0.34509805 0.42352942]\n   [0.23529412 0.25882354 0.33725491]\n   ...\n   [0.31764707 0.31764707 0.35686275]\n   [0.27450982 0.29411766 0.30980393]\n   [0.3137255  0.32941177 0.36470589]]\n\n  [[0.         0.         0.        ]\n   [0.34509805 0.37254903 0.42745098]\n   [0.25490198 0.28235295 0.32549021]\n   ...\n   [0.31764707 0.32941177 0.35294119]\n   [0.29803923 0.3137255  0.33725491]\n   [0.30588236 0.30980393 0.34117648]]\n\n  [[0.         0.         0.        ]\n   [0.25490198 0.27843139 0.3137255 ]\n   [0.29411766 0.32156864 0.36470589]\n   ...\n   [0.32156864 0.33333334 0.36470589]\n   [0.29019609 0.30980393 0.35294119]\n   [0.28627452 0.3019608  0.34509805]]]\n\n\n [[[0.10980392 0.13725491 0.18039216]\n   [0.15294118 0.18431373 0.23529412]\n   [0.15686275 0.16470589 0.20784314]\n   ...\n   [0.96862745 0.97647059 0.98823529]\n   [0.90980393 0.9137255  0.92941177]\n   [0.84705883 0.8509804  0.87450981]]\n\n  [[0.2        0.24705882 0.32941177]\n   [0.23529412 0.28235295 0.36470589]\n   [0.23921569 0.25490198 0.31764707]\n   ...\n   [0.78431374 0.7764706  0.78823531]\n   [0.94901961 0.94901961 0.95294118]\n   [0.90980393 0.90588236 0.92156863]]\n\n  [[0.15294118 0.18431373 0.24705882]\n   [0.14901961 0.17254902 0.23529412]\n   [0.16078432 0.16078432 0.20784314]\n   ...\n   [0.13725491 0.10980392 0.13333334]\n   [0.66274512 0.65098041 0.66666669]\n   [0.94117647 0.94509804 0.95686275]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.34901962 0.43529412 0.50588238]\n   [0.32549021 0.38039216 0.45882353]\n   ...\n   [0.38039216 0.36078432 0.43137255]\n   [0.33725491 0.3019608  0.3882353 ]\n   [0.38039216 0.32941177 0.42352942]]\n\n  [[0.         0.         0.        ]\n   [0.38039216 0.44705883 0.52156866]\n   [0.34509805 0.41176471 0.48235294]\n   ...\n   [0.38039216 0.33333334 0.42352942]\n   [0.39607844 0.33725491 0.43137255]\n   [0.39607844 0.33725491 0.42745098]]\n\n  [[0.         0.         0.        ]\n   [0.25882354 0.29803923 0.34509805]\n   [0.32549021 0.38431373 0.44705883]\n   ...\n   [0.40392157 0.34117648 0.44313726]\n   [0.39215687 0.32941177 0.42352942]\n   [0.3882353  0.32941177 0.41960785]]]\n\n\n [[[0.23137255 0.28235295 0.3019608 ]\n   [0.3137255  0.36862746 0.40000001]\n   [0.31764707 0.36470589 0.40000001]\n   ...\n   [0.47450981 0.59607846 0.70588237]\n   [0.36862746 0.49019608 0.60784316]\n   [0.25490198 0.38039216 0.49019608]]\n\n  [[0.41568628 0.50588238 0.56470591]\n   [0.48627451 0.57254905 0.64313728]\n   [0.48235294 0.56078434 0.63137257]\n   ...\n   [0.40000001 0.48235294 0.59607846]\n   [0.45882353 0.54901963 0.66666669]\n   [0.32941177 0.43529412 0.53725493]]\n\n  [[0.32156864 0.38039216 0.43921569]\n   [0.3019608  0.36470589 0.42352942]\n   [0.29803923 0.35294119 0.41568628]\n   ...\n   [0.00784314 0.03137255 0.14509805]\n   [0.24313726 0.29803923 0.42352942]\n   [0.3764706  0.46666667 0.58039218]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.49019608 0.35686275 0.48235294]\n   [0.3882353  0.27450982 0.3764706 ]\n   ...\n   [0.11764706 0.19607843 0.24313726]\n   [0.07450981 0.14509805 0.19215687]\n   [0.10588235 0.16470589 0.22352941]]\n\n  [[0.         0.         0.        ]\n   [0.50588238 0.38039216 0.49803922]\n   [0.36862746 0.25098041 0.36078432]\n   ...\n   [0.09411765 0.15686275 0.21960784]\n   [0.10196079 0.16078432 0.22745098]\n   [0.10588235 0.16862746 0.22352941]]\n\n  [[0.         0.         0.        ]\n   [0.36078432 0.29411766 0.36470589]\n   [0.43921569 0.33725491 0.44313726]\n   ...\n   [0.10980392 0.17647059 0.23921569]\n   [0.10980392 0.17647059 0.23921569]\n   [0.10980392 0.17647059 0.23921569]]]\n\n\n ...\n\n\n [[[0.21568628 0.17254902 0.2       ]\n   [0.32156864 0.23529412 0.24313726]\n   [0.36862746 0.22352941 0.24705882]\n   ...\n   [0.85882354 0.78823531 0.8392157 ]\n   [0.79215688 0.69803923 0.73725492]\n   [0.6901961  0.57647061 0.6156863 ]]\n\n  [[0.39215687 0.32549021 0.36862746]\n   [0.47450981 0.35294119 0.34901962]\n   [0.65882355 0.35686275 0.40000001]\n   ...\n   [0.64705884 0.63529414 0.71372551]\n   [0.80784315 0.76078433 0.81960785]\n   [0.73333335 0.63921571 0.68235296]]\n\n  [[0.29411766 0.24705882 0.27843139]\n   [0.28235295 0.21960784 0.20784314]\n   [0.47450981 0.23137255 0.25098041]\n   ...\n   [0.05882353 0.09803922 0.23137255]\n   [0.45490196 0.43921569 0.53333336]\n   [0.73333335 0.66666669 0.73725492]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.36470589 0.27843139 0.47058824]\n   [0.33725491 0.23137255 0.41568628]\n   ...\n   [0.00392157 0.3137255  0.65490198]\n   [0.         0.26666668 0.59607846]\n   [0.         0.30588236 0.64705884]]\n\n  [[0.         0.         0.        ]\n   [0.35294119 0.30980393 0.51372552]\n   [0.30588236 0.23529412 0.45490196]\n   ...\n   [0.00392157 0.26274511 0.6156863 ]\n   [0.01960784 0.25882354 0.61176473]\n   [0.00784314 0.27450982 0.63137257]]\n\n  [[0.         0.         0.        ]\n   [0.24705882 0.28235295 0.39215687]\n   [0.30980393 0.35294119 0.52941179]\n   ...\n   [0.02745098 0.25882354 0.61960787]\n   [0.03921569 0.24705882 0.60000002]\n   [0.03137255 0.25490198 0.60784316]]]\n\n\n [[[0.30588236 0.29803923 0.30980393]\n   [0.40000001 0.40000001 0.39607844]\n   [0.40000001 0.40000001 0.40000001]\n   ...\n   [0.81568629 0.98039216 1.        ]\n   [0.73333335 0.92941177 1.        ]\n   [0.60784316 0.8509804  0.98823529]]\n\n  [[0.58039218 0.5411765  0.60784316]\n   [0.64313728 0.63137257 0.68627453]\n   [0.59215689 0.58823532 0.627451  ]\n   ...\n   [0.64313728 0.80784315 0.94509804]\n   [0.78431374 0.9254902  0.98039216]\n   [0.67843139 0.89019608 0.99215686]]\n\n  [[0.4509804  0.39215687 0.47058824]\n   [0.44705883 0.40000001 0.47843137]\n   [0.42745098 0.38431373 0.44313726]\n   ...\n   [0.03137255 0.20392157 0.3764706 ]\n   [0.45882353 0.63529414 0.7764706 ]\n   [0.74901962 0.9254902  1.        ]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.1882353  0.26274511 0.43529412]\n   [0.16862746 0.22745098 0.38431373]\n   ...\n   [0.16078432 0.17647059 0.10980392]\n   [0.1254902  0.12941177 0.06666667]\n   [0.15294118 0.15686275 0.09411765]]\n\n  [[0.         0.         0.        ]\n   [0.21176471 0.32549021 0.49803922]\n   [0.13725491 0.24313726 0.40000001]\n   ...\n   [0.16470589 0.17254902 0.12941177]\n   [0.17647059 0.17254902 0.1254902 ]\n   [0.18039216 0.16862746 0.12156863]]\n\n  [[0.         0.         0.        ]\n   [0.15686275 0.26666668 0.36862746]\n   [0.18039216 0.32549021 0.4627451 ]\n   ...\n   [0.19215687 0.18431373 0.12941177]\n   [0.18039216 0.16470589 0.1254902 ]\n   [0.18039216 0.16470589 0.12156863]]]\n\n\n [[[0.21568628 0.22352941 0.30980393]\n   [0.30588236 0.35294119 0.39607844]\n   [0.28235295 0.38431373 0.40000001]\n   ...\n   [0.66274512 0.67450982 0.64705884]\n   [0.58039218 0.58039218 0.53725493]\n   [0.47450981 0.47843137 0.41960785]]\n\n  [[0.42745098 0.43529412 0.62352943]\n   [0.47058824 0.54901963 0.67843139]\n   [0.40000001 0.56862748 0.63529414]\n   ...\n   [0.52549022 0.57647061 0.57647061]\n   [0.66666669 0.6901961  0.66274512]\n   [0.54509807 0.54901963 0.50588238]]\n\n  [[0.39607844 0.41960785 0.57254905]\n   [0.3137255  0.38431373 0.50980395]\n   [0.27450982 0.38431373 0.4627451 ]\n   ...\n   [0.01960784 0.08627451 0.11764706]\n   [0.32549021 0.36078432 0.35294119]\n   [0.53725493 0.5411765  0.50588238]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.22745098 0 [Op:__inference_one_step_on_iterator_8525]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_68393/1343263063.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m vae.fit(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_generator_Mapa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float64,), but the yielded element was [[[[0.20392157 0.17254902 0.18431373]\n   [0.24705882 0.23529412 0.23921569]\n   [0.20392157 0.20784314 0.20392157]\n   ...\n   [0.99607843 0.98823529 0.98039216]\n   [0.98823529 0.98039216 0.97254902]\n   [0.96862745 0.95294118 0.93725491]]\n\n  [[0.38431373 0.32549021 0.34509805]\n   [0.34509805 0.33725491 0.33725491]\n   [0.34117648 0.35686275 0.34901962]\n   ...\n   [0.78823531 0.78039217 0.7764706 ]\n   [0.94117647 0.94117647 0.93725491]\n   [0.98039216 0.97647059 0.96862745]]\n\n  [[0.3137255  0.25490198 0.27450982]\n   [0.15686275 0.15686275 0.15686275]\n   [0.26666668 0.28235295 0.27058825]\n   ...\n   [0.20392157 0.18431373 0.20392157]\n   [0.70980394 0.7019608  0.70980394]\n   [1.         1.         1.        ]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.31764707 0.34509805 0.42352942]\n   [0.23529412 0.25882354 0.33725491]\n   ...\n   [0.31764707 0.31764707 0.35686275]\n   [0.27450982 0.29411766 0.30980393]\n   [0.3137255  0.32941177 0.36470589]]\n\n  [[0.         0.         0.        ]\n   [0.34509805 0.37254903 0.42745098]\n   [0.25490198 0.28235295 0.32549021]\n   ...\n   [0.31764707 0.32941177 0.35294119]\n   [0.29803923 0.3137255  0.33725491]\n   [0.30588236 0.30980393 0.34117648]]\n\n  [[0.         0.         0.        ]\n   [0.25490198 0.27843139 0.3137255 ]\n   [0.29411766 0.32156864 0.36470589]\n   ...\n   [0.32156864 0.33333334 0.36470589]\n   [0.29019609 0.30980393 0.35294119]\n   [0.28627452 0.3019608  0.34509805]]]\n\n\n [[[0.10980392 0.13725491 0.18039216]\n   [0.15294118 0.18431373 0.23529412]\n   [0.15686275 0.16470589 0.20784314]\n   ...\n   [0.96862745 0.97647059 0.98823529]\n   [0.90980393 0.9137255  0.92941177]\n   [0.84705883 0.8509804  0.87450981]]\n\n  [[0.2        0.24705882 0.32941177]\n   [0.23529412 0.28235295 0.36470589]\n   [0.23921569 0.25490198 0.31764707]\n   ...\n   [0.78431374 0.7764706  0.78823531]\n   [0.94901961 0.94901961 0.95294118]\n   [0.90980393 0.90588236 0.92156863]]\n\n  [[0.15294118 0.18431373 0.24705882]\n   [0.14901961 0.17254902 0.23529412]\n   [0.16078432 0.16078432 0.20784314]\n   ...\n   [0.13725491 0.10980392 0.13333334]\n   [0.66274512 0.65098041 0.66666669]\n   [0.94117647 0.94509804 0.95686275]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.34901962 0.43529412 0.50588238]\n   [0.32549021 0.38039216 0.45882353]\n   ...\n   [0.38039216 0.36078432 0.43137255]\n   [0.33725491 0.3019608  0.3882353 ]\n   [0.38039216 0.32941177 0.42352942]]\n\n  [[0.         0.         0.        ]\n   [0.38039216 0.44705883 0.52156866]\n   [0.34509805 0.41176471 0.48235294]\n   ...\n   [0.38039216 0.33333334 0.42352942]\n   [0.39607844 0.33725491 0.43137255]\n   [0.39607844 0.33725491 0.42745098]]\n\n  [[0.         0.         0.        ]\n   [0.25882354 0.29803923 0.34509805]\n   [0.32549021 0.38431373 0.44705883]\n   ...\n   [0.40392157 0.34117648 0.44313726]\n   [0.39215687 0.32941177 0.42352942]\n   [0.3882353  0.32941177 0.41960785]]]\n\n\n [[[0.23137255 0.28235295 0.3019608 ]\n   [0.3137255  0.36862746 0.40000001]\n   [0.31764707 0.36470589 0.40000001]\n   ...\n   [0.47450981 0.59607846 0.70588237]\n   [0.36862746 0.49019608 0.60784316]\n   [0.25490198 0.38039216 0.49019608]]\n\n  [[0.41568628 0.50588238 0.56470591]\n   [0.48627451 0.57254905 0.64313728]\n   [0.48235294 0.56078434 0.63137257]\n   ...\n   [0.40000001 0.48235294 0.59607846]\n   [0.45882353 0.54901963 0.66666669]\n   [0.32941177 0.43529412 0.53725493]]\n\n  [[0.32156864 0.38039216 0.43921569]\n   [0.3019608  0.36470589 0.42352942]\n   [0.29803923 0.35294119 0.41568628]\n   ...\n   [0.00784314 0.03137255 0.14509805]\n   [0.24313726 0.29803923 0.42352942]\n   [0.3764706  0.46666667 0.58039218]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.49019608 0.35686275 0.48235294]\n   [0.3882353  0.27450982 0.3764706 ]\n   ...\n   [0.11764706 0.19607843 0.24313726]\n   [0.07450981 0.14509805 0.19215687]\n   [0.10588235 0.16470589 0.22352941]]\n\n  [[0.         0.         0.        ]\n   [0.50588238 0.38039216 0.49803922]\n   [0.36862746 0.25098041 0.36078432]\n   ...\n   [0.09411765 0.15686275 0.21960784]\n   [0.10196079 0.16078432 0.22745098]\n   [0.10588235 0.16862746 0.22352941]]\n\n  [[0.         0.         0.        ]\n   [0.36078432 0.29411766 0.36470589]\n   [0.43921569 0.33725491 0.44313726]\n   ...\n   [0.10980392 0.17647059 0.23921569]\n   [0.10980392 0.17647059 0.23921569]\n   [0.10980392 0.17647059 0.23921569]]]\n\n\n ...\n\n\n [[[0.21568628 0.17254902 0.2       ]\n   [0.32156864 0.23529412 0.24313726]\n   [0.36862746 0.22352941 0.24705882]\n   ...\n   [0.85882354 0.78823531 0.8392157 ]\n   [0.79215688 0.69803923 0.73725492]\n   [0.6901961  0.57647061 0.6156863 ]]\n\n  [[0.39215687 0.32549021 0.36862746]\n   [0.47450981 0.35294119 0.34901962]\n   [0.65882355 0.35686275 0.40000001]\n   ...\n   [0.64705884 0.63529414 0.71372551]\n   [0.80784315 0.76078433 0.81960785]\n   [0.73333335 0.63921571 0.68235296]]\n\n  [[0.29411766 0.24705882 0.27843139]\n   [0.28235295 0.21960784 0.20784314]\n   [0.47450981 0.23137255 0.25098041]\n   ...\n   [0.05882353 0.09803922 0.23137255]\n   [0.45490196 0.43921569 0.53333336]\n   [0.73333335 0.66666669 0.73725492]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.36470589 0.27843139 0.47058824]\n   [0.33725491 0.23137255 0.41568628]\n   ...\n   [0.00392157 0.3137255  0.65490198]\n   [0.         0.26666668 0.59607846]\n   [0.         0.30588236 0.64705884]]\n\n  [[0.         0.         0.        ]\n   [0.35294119 0.30980393 0.51372552]\n   [0.30588236 0.23529412 0.45490196]\n   ...\n   [0.00392157 0.26274511 0.6156863 ]\n   [0.01960784 0.25882354 0.61176473]\n   [0.00784314 0.27450982 0.63137257]]\n\n  [[0.         0.         0.        ]\n   [0.24705882 0.28235295 0.39215687]\n   [0.30980393 0.35294119 0.52941179]\n   ...\n   [0.02745098 0.25882354 0.61960787]\n   [0.03921569 0.24705882 0.60000002]\n   [0.03137255 0.25490198 0.60784316]]]\n\n\n [[[0.30588236 0.29803923 0.30980393]\n   [0.40000001 0.40000001 0.39607844]\n   [0.40000001 0.40000001 0.40000001]\n   ...\n   [0.81568629 0.98039216 1.        ]\n   [0.73333335 0.92941177 1.        ]\n   [0.60784316 0.8509804  0.98823529]]\n\n  [[0.58039218 0.5411765  0.60784316]\n   [0.64313728 0.63137257 0.68627453]\n   [0.59215689 0.58823532 0.627451  ]\n   ...\n   [0.64313728 0.80784315 0.94509804]\n   [0.78431374 0.9254902  0.98039216]\n   [0.67843139 0.89019608 0.99215686]]\n\n  [[0.4509804  0.39215687 0.47058824]\n   [0.44705883 0.40000001 0.47843137]\n   [0.42745098 0.38431373 0.44313726]\n   ...\n   [0.03137255 0.20392157 0.3764706 ]\n   [0.45882353 0.63529414 0.7764706 ]\n   [0.74901962 0.9254902  1.        ]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.1882353  0.26274511 0.43529412]\n   [0.16862746 0.22745098 0.38431373]\n   ...\n   [0.16078432 0.17647059 0.10980392]\n   [0.1254902  0.12941177 0.06666667]\n   [0.15294118 0.15686275 0.09411765]]\n\n  [[0.         0.         0.        ]\n   [0.21176471 0.32549021 0.49803922]\n   [0.13725491 0.24313726 0.40000001]\n   ...\n   [0.16470589 0.17254902 0.12941177]\n   [0.17647059 0.17254902 0.1254902 ]\n   [0.18039216 0.16862746 0.12156863]]\n\n  [[0.         0.         0.        ]\n   [0.15686275 0.26666668 0.36862746]\n   [0.18039216 0.32549021 0.4627451 ]\n   ...\n   [0.19215687 0.18431373 0.12941177]\n   [0.18039216 0.16470589 0.1254902 ]\n   [0.18039216 0.16470589 0.12156863]]]\n\n\n [[[0.21568628 0.22352941 0.30980393]\n   [0.30588236 0.35294119 0.39607844]\n   [0.28235295 0.38431373 0.40000001]\n   ...\n   [0.66274512 0.67450982 0.64705884]\n   [0.58039218 0.58039218 0.53725493]\n   [0.47450981 0.47843137 0.41960785]]\n\n  [[0.42745098 0.43529412 0.62352943]\n   [0.47058824 0.54901963 0.67843139]\n   [0.40000001 0.56862748 0.63529414]\n   ...\n   [0.52549022 0.57647061 0.57647061]\n   [0.66666669 0.6901961  0.66274512]\n   [0.54509807 0.54901963 0.50588238]]\n\n  [[0.39607844 0.41960785 0.57254905]\n   [0.3137255  0.38431373 0.50980395]\n   [0.27450982 0.38431373 0.4627451 ]\n   ...\n   [0.01960784 0.08627451 0.11764706]\n   [0.32549021 0.36078432 0.35294119]\n   [0.53725493 0.5411765  0.50588238]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.22745098 0 [Op:__inference_one_step_on_iterator_8525]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0784314]\n",
      "   ...\n",
      "   [0.96862745 0.97647059 0.98823529]\n",
      "   [0.90980393 0.9137255  0.92941177]\n",
      "   [0.84705883 0.8509804  0.87450981]]\n",
      "\n",
      "  [[0.2        0.24705882 0.32941177]\n",
      "   [0.23529412 0.28235295 0.36470589]\n",
      "   [0.23921569 0.25490198 0.31764707]\n",
      "   ...\n",
      "   [0.78431374 0.7764706  0.78823531]\n",
      "   [0.94901961 0.94901961 0.95294118]\n",
      "   [0.90980393 0.90588236 0.92156863]]\n",
      "\n",
      "  [[0.15294118 0.18431373 0.24705882]\n",
      "   [0.14901961 0.17254902 0.23529412]\n",
      "   [0.16078432 0.16078432 0.20784314]\n",
      "   ...\n",
      "   [0.13725491 0.10980392 0.13333334]\n",
      "   [0.66274512 0.65098041 0.66666669]\n",
      "   [0.94117647 0.94509804 0.95686275]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.34901962 0.43529412 0.50588238]\n",
      "   [0.32549021 0.38039216 0.45882353]\n",
      "   ...\n",
      "   [0.38039216 0.36078432 0.43137255]\n",
      "   [0.33725491 0.3019608  0.3882353 ]\n",
      "   [0.38039216 0.32941177 0.42352942]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.38039216 0.44705883 0.52156866]\n",
      "   [0.34509805 0.41176471 0.48235294]\n",
      "   ...\n",
      "   [0.38039216 0.33333334 0.42352942]\n",
      "   [0.39607844 0.33725491 0.43137255]\n",
      "   [0.39607844 0.33725491 0.42745098]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.25882354 0.29803923 0.34509805]\n",
      "   [0.32549021 0.38431373 0.44705883]\n",
      "   ...\n",
      "   [0.40392157 0.34117648 0.44313726]\n",
      "   [0.39215687 0.32941177 0.42352942]\n",
      "   [0.3882353  0.32941177 0.41960785]]]\n",
      "\n",
      "\n",
      " [[[0.23137255 0.28235295 0.3019608 ]\n",
      "   [0.3137255  0.36862746 0.40000001]\n",
      "   [0.31764707 0.36470589 0.40000001]\n",
      "   ...\n",
      "   [0.47450981 0.59607846 0.70588237]\n",
      "   [0.36862746 0.49019608 0.60784316]\n",
      "   [0.25490198 0.38039216 0.49019608]]\n",
      "\n",
      "  [[0.41568628 0.50588238 0.56470591]\n",
      "   [0.48627451 0.57254905 0.64313728]\n",
      "   [0.48235294 0.56078434 0.63137257]\n",
      "   ...\n",
      "   [0.40000001 0.48235294 0.59607846]\n",
      "   [0.45882353 0.54901963 0.66666669]\n",
      "   [0.32941177 0.43529412 0.53725493]]\n",
      "\n",
      "  [[0.32156864 0.38039216 0.43921569]\n",
      "   [0.3019608  0.36470589 0.42352942]\n",
      "   [0.29803923 0.35294119 0.41568628]\n",
      "   ...\n",
      "   [0.00784314 0.03137255 0.14509805]\n",
      "   [0.24313726 0.29803923 0.42352942]\n",
      "   [0.3764706  0.46666667 0.58039218]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.49019608 0.35686275 0.48235294]\n",
      "   [0.3882353  0.27450982 0.3764706 ]\n",
      "   ...\n",
      "   [0.11764706 0.19607843 0.24313726]\n",
      "   [0.07450981 0.14509805 0.19215687]\n",
      "   [0.10588235 0.16470589 0.22352941]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.50588238 0.38039216 0.49803922]\n",
      "   [0.36862746 0.25098041 0.36078432]\n",
      "   ...\n",
      "   [0.09411765 0.15686275 0.21960784]\n",
      "   [0.10196079 0.16078432 0.22745098]\n",
      "   [0.10588235 0.16862746 0.22352941]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.36078432 0.29411766 0.36470589]\n",
      "   [0.43921569 0.33725491 0.44313726]\n",
      "   ...\n",
      "   [0.10980392 0.17647059 0.23921569]\n",
      "   [0.10980392 0.17647059 0.23921569]\n",
      "   [0.10980392 0.17647059 0.23921569]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.21568628 0.17254902 0.2       ]\n",
      "   [0.32156864 0.23529412 0.24313726]\n",
      "   [0.36862746 0.22352941 0.24705882]\n",
      "   ...\n",
      "   [0.85882354 0.78823531 0.8392157 ]\n",
      "   [0.79215688 0.69803923 0.73725492]\n",
      "   [0.6901961  0.57647061 0.6156863 ]]\n",
      "\n",
      "  [[0.39215687 0.32549021 0.36862746]\n",
      "   [0.47450981 0.35294119 0.34901962]\n",
      "   [0.65882355 0.35686275 0.40000001]\n",
      "   ...\n",
      "   [0.64705884 0.63529414 0.71372551]\n",
      "   [0.80784315 0.76078433 0.81960785]\n",
      "   [0.73333335 0.63921571 0.68235296]]\n",
      "\n",
      "  [[0.29411766 0.24705882 0.27843139]\n",
      "   [0.28235295 0.21960784 0.20784314]\n",
      "   [0.47450981 0.23137255 0.25098041]\n",
      "   ...\n",
      "   [0.05882353 0.09803922 0.23137255]\n",
      "   [0.45490196 0.43921569 0.53333336]\n",
      "   [0.73333335 0.66666669 0.73725492]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.36470589 0.27843139 0.47058824]\n",
      "   [0.33725491 0.23137255 0.41568628]\n",
      "   ...\n",
      "   [0.00392157 0.3137255  0.65490198]\n",
      "   [0.         0.26666668 0.59607846]\n",
      "   [0.         0.30588236 0.64705884]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.35294119 0.30980393 0.51372552]\n",
      "   [0.30588236 0.23529412 0.45490196]\n",
      "   ...\n",
      "   [0.00392157 0.26274511 0.6156863 ]\n",
      "   [0.01960784 0.25882354 0.61176473]\n",
      "   [0.00784314 0.27450982 0.63137257]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.24705882 0.28235295 0.39215687]\n",
      "   [0.30980393 0.35294119 0.52941179]\n",
      "   ...\n",
      "   [0.02745098 0.25882354 0.61960787]\n",
      "   [0.03921569 0.24705882 0.60000002]\n",
      "   [0.03137255 0.25490198 0.60784316]]]\n",
      "\n",
      "\n",
      " [[[0.30588236 0.29803923 0.30980393]\n",
      "   [0.40000001 0.40000001 0.39607844]\n",
      "   [0.40000001 0.40000001 0.40000001]\n",
      "   ...\n",
      "   [0.81568629 0.98039216 1.        ]\n",
      "   [0.73333335 0.92941177 1.        ]\n",
      "   [0.60784316 0.8509804  0.98823529]]\n",
      "\n",
      "  [[0.58039218 0.5411765  0.60784316]\n",
      "   [0.64313728 0.63137257 0.68627453]\n",
      "   [0.59215689 0.58823532 0.627451  ]\n",
      "   ...\n",
      "   [0.64313728 0.80784315 0.94509804]\n",
      "   [0.78431374 0.9254902  0.98039216]\n",
      "   [0.67843139 0.89019608 0.99215686]]\n",
      "\n",
      "  [[0.4509804  0.39215687 0.47058824]\n",
      "   [0.44705883 0.40000001 0.47843137]\n",
      "   [0.42745098 0.38431373 0.44313726]\n",
      "   ...\n",
      "   [0.03137255 0.20392157 0.3764706 ]\n",
      "   [0.45882353 0.63529414 0.7764706 ]\n",
      "   [0.74901962 0.9254902  1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.1882353  0.26274511 0.43529412]\n",
      "   [0.16862746 0.22745098 0.38431373]\n",
      "   ...\n",
      "   [0.16078432 0.17647059 0.10980392]\n",
      "   [0.1254902  0.12941177 0.06666667]\n",
      "   [0.15294118 0.15686275 0.09411765]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.21176471 0.32549021 0.49803922]\n",
      "   [0.13725491 0.24313726 0.40000001]\n",
      "   ...\n",
      "   [0.16470589 0.17254902 0.12941177]\n",
      "   [0.17647059 0.17254902 0.1254902 ]\n",
      "   [0.18039216 0.16862746 0.12156863]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.15686275 0.26666668 0.36862746]\n",
      "   [0.18039216 0.32549021 0.4627451 ]\n",
      "   ...\n",
      "   [0.19215687 0.18431373 0.12941177]\n",
      "   [0.18039216 0.16470589 0.1254902 ]\n",
      "   [0.18039216 0.16470589 0.12156863]]]\n",
      "\n",
      "\n",
      " [[[0.21568628 0.22352941 0.30980393]\n",
      "   [0.30588236 0.35294119 0.39607844]\n",
      "   [0.28235295 0.38431373 0.40000001]\n",
      "   ...\n",
      "   [0.66274512 0.67450982 0.64705884]\n",
      "   [0.58039218 0.58039218 0.53725493]\n",
      "   [0.47450981 0.47843137 0.41960785]]\n",
      "\n",
      "  [[0.42745098 0.43529412 0.62352943]\n",
      "   [0.47058824 0.54901963 0.67843139]\n",
      "   [0.40000001 0.56862748 0.63529414]\n",
      "   ...\n",
      "   [0.52549022 0.57647061 0.57647061]\n",
      "   [0.66666669 0.6901961  0.66274512]\n",
      "   [0.54509807 0.54901963 0.50588238]]\n",
      "\n",
      "  [[0.39607844 0.41960785 0.57254905]\n",
      "   [0.3137255  0.38431373 0.50980395]\n",
      "   [0.27450982 0.38431373 0.4627451 ]\n",
      "   ...\n",
      "   [0.01960784 0.08627451 0.11764706]\n",
      "   [0.32549021 0.36078432 0.35294119]\n",
      "   [0.53725493 0.5411765  0.50588238]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.22745098 0.29411766 0.27843139]\n",
      "   [0.23529412 0.26274511 0.25098041]\n",
      "   ...\n",
      "   [0.12941177 0.27058825 0.20784314]\n",
      "   [0.07843138 0.21960784 0.16078432]\n",
      "   [0.11764706 0.23921569 0.20392157]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.21176471 0.34901962 0.34901962]\n",
      "   [0.16470589 0.28627452 0.29411766]\n",
      "   ...\n",
      "   [0.18039216 0.24705882 0.21568628]\n",
      "   [0.2        0.25098041 0.23529412]\n",
      "   [0.19607843 0.25490198 0.21960784]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.18039216 0.29019609 0.29803923]\n",
      "   [0.23137255 0.38039216 0.38039216]\n",
      "   ...\n",
      "   [0.22352941 0.26274511 0.23921569]\n",
      "   [0.22745098 0.25098041 0.23921569]\n",
      "   [0.22745098 0.25882354 0.23529412]]]].\n",
      "\n",
      "\n",
      "2024-11-01 22:38:29.439079: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float64,), but the yielded element was [[[[0.20392157 0.17254902 0.18431373]\n",
      "   [0.24705882 0.23529412 0.23921569]\n",
      "   [0.20392157 0.20784314 0.20392157]\n",
      "   ...\n",
      "   [0.99607843 0.98823529 0.98039216]\n",
      "   [0.98823529 0.98039216 0.97254902]\n",
      "   [0.96862745 0.95294118 0.93725491]]\n",
      "\n",
      "  [[0.38431373 0.32549021 0.34509805]\n",
      "   [0.34509805 0.33725491 0.33725491]\n",
      "   [0.34117648 0.35686275 0.34901962]\n",
      "   ...\n",
      "   [0.78823531 0.78039217 0.7764706 ]\n",
      "   [0.94117647 0.94117647 0.93725491]\n",
      "   [0.98039216 0.97647059 0.96862745]]\n",
      "\n",
      "  [[0.3137255  0.25490198 0.27450982]\n",
      "   [0.15686275 0.15686275 0.15686275]\n",
      "   [0.26666668 0.28235295 0.27058825]\n",
      "   ...\n",
      "   [0.20392157 0.18431373 0.20392157]\n",
      "   [0.70980394 0.7019608  0.70980394]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.31764707 0.34509805 0.42352942]\n",
      "   [0.23529412 0.25882354 0.33725491]\n",
      "   ...\n",
      "   [0.31764707 0.31764707 0.35686275]\n",
      "   [0.27450982 0.29411766 0.30980393]\n",
      "   [0.3137255  0.32941177 0.36470589]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.34509805 0.37254903 0.42745098]\n",
      "   [0.25490198 0.28235295 0.32549021]\n",
      "   ...\n",
      "   [0.31764707 0.32941177 0.35294119]\n",
      "   [0.29803923 0.3137255  0.33725491]\n",
      "   [0.30588236 0.30980393 0.34117648]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.25490198 0.27843139 0.3137255 ]\n",
      "   [0.29411766 0.32156864 0.36470589]\n",
      "   ...\n",
      "   [0.32156864 0.33333334 0.36470589]\n",
      "   [0.29019609 0.30980393 0.35294119]\n",
      "   [0.28627452 0.3019608  0.34509805]]]\n",
      "\n",
      "\n",
      " [[[0.10980392 0.13725491 0.18039216]\n",
      "   [0.15294118 0.18431373 0.23529412]\n",
      "   [0.15686275 0.16470589 0.20784314]\n",
      "   ...\n",
      "   [0.96862745 0.97647059 0.98823529]\n",
      "   [0.90980393 0.9137255  0.92941177]\n",
      "   [0.84705883 0.8509804  0.87450981]]\n",
      "\n",
      "  [[0.2        0.24705882 0.32941177]\n",
      "   [0.23529412 0.28235295 0.36470589]\n",
      "   [0.23921569 0.25490198 0.31764707]\n",
      "   ...\n",
      "   [0.78431374 0.7764706  0.78823531]\n",
      "   [0.94901961 0.94901961 0.95294118]\n",
      "   [0.90980393 0.90588236 0.92156863]]\n",
      "\n",
      "  [[0.15294118 0.18431373 0.24705882]\n",
      "   [0.14901961 0.17254902 0.23529412]\n",
      "   [0.16078432 0.16078432 0.20784314]\n",
      "   ...\n",
      "   [0.13725491 0.10980392 0.13333334]\n",
      "   [0.66274512 0.65098041 0.66666669]\n",
      "   [0.94117647 0.94509804 0.95686275]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.34901962 0.43529412 0.50588238]\n",
      "   [0.32549021 0.38039216 0.45882353]\n",
      "   ...\n",
      "   [0.38039216 0.36078432 0.43137255]\n",
      "   [0.33725491 0.3019608  0.3882353 ]\n",
      "   [0.38039216 0.32941177 0.42352942]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.38039216 0.44705883 0.52156866]\n",
      "   [0.34509805 0.41176471 0.48235294]\n",
      "   ...\n",
      "   [0.38039216 0.33333334 0.42352942]\n",
      "   [0.39607844 0.33725491 0.43137255]\n",
      "   [0.39607844 0.33725491 0.42745098]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.25882354 0.29803923 0.34509805]\n",
      "   [0.32549021 0.38431373 0.44705883]\n",
      "   ...\n",
      "   [0.40392157 0.34117648 0.44313726]\n",
      "   [0.39215687 0.32941177 0.42352942]\n",
      "   [0.3882353  0.32941177 0.41960785]]]\n",
      "\n",
      "\n",
      " [[[0.23137255 0.28235295 0.3019608 ]\n",
      "   [0.3137255  0.36862746 0.40000001]\n",
      "   [0.31764707 0.36470589 0.40000001]\n",
      "   ...\n",
      "   [0.47450981 0.59607846 0.70588237]\n",
      "   [0.36862746 0.49019608 0.60784316]\n",
      "   [0.25490198 0.38039216 0.49019608]]\n",
      "\n",
      "  [[0.41568628 0.50588238 0.56470591]\n",
      "   [0.48627451 0.57254905 0.64313728]\n",
      "   [0.48235294 0.56078434 0.63137257]\n",
      "   ...\n",
      "   [0.40000001 0.48235294 0.59607846]\n",
      "   [0.45882353 0.54901963 0.66666669]\n",
      "   [0.32941177 0.43529412 0.53725493]]\n",
      "\n",
      "  [[0.32156864 0.38039216 0.43921569]\n",
      "   [0.3019608  0.36470589 0.42352942]\n",
      "   [0.29803923 0.35294119 0.41568628]\n",
      "   ...\n",
      "   [0.00784314 0.03137255 0.14509805]\n",
      "   [0.24313726 0.29803923 0.42352942]\n",
      "   [0.3764706  0.46666667 0.58039218]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.49019608 0.35686275 0.48235294]\n",
      "   [0.3882353  0.27450982 0.3764706 ]\n",
      "   ...\n",
      "   [0.11764706 0.19607843 0.24313726]\n",
      "   [0.07450981 0.14509805 0.19215687]\n",
      "   [0.10588235 0.16470589 0.22352941]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.50588238 0.38039216 0.49803922]\n",
      "   [0.36862746 0.25098041 0.36078432]\n",
      "   ...\n",
      "   [0.09411765 0.15686275 0.21960784]\n",
      "   [0.10196079 0.16078432 0.22745098]\n",
      "   [0.10588235 0.16862746 0.22352941]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.36078432 0.29411766 0.36470589]\n",
      "   [0.43921569 0.33725491 0.44313726]\n",
      "   ...\n",
      "   [0.10980392 0.17647059 0.23921569]\n",
      "   [0.10980392 0.17647059 0.23921569]\n",
      "   [0.10980392 0.17647059 0.23921569]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.21568628 0.17254902 0.2       ]\n",
      "   [0.32156864 0.23529412 0.24313726]\n",
      "   [0.36862746 0.22352941 0.24705882]\n",
      "   ...\n",
      "   [0.85882354 0.78823531 0.8392157 ]\n",
      "   [0.79215688 0.69803923 0.73725492]\n",
      "   [0.6901961  0.57647061 0.6156863 ]]\n",
      "\n",
      "  [[0.39215687 0.32549021 0.36862746]\n",
      "   [0.47450981 0.35294119 0.34901962]\n",
      "   [0.65882355 0.35686275 0.40000001]\n",
      "   ...\n",
      "   [0.64705884 0.63529414 0.71372551]\n",
      "   [0.80784315 0.76078433 0.81960785]\n",
      "   [0.73333335 0.63921571 0.68235296]]\n",
      "\n",
      "  [[0.29411766 0.24705882 0.27843139]\n",
      "   [0.28235295 0.21960784 0.20784314]\n",
      "   [0.47450981 0.23137255 0.25098041]\n",
      "   ...\n",
      "   [0.05882353 0.09803922 0.23137255]\n",
      "   [0.45490196 0.43921569 0.53333336]\n",
      "   [0.73333335 0.66666669 0.73725492]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.36470589 0.27843139 0.47058824]\n",
      "   [0.33725491 0.23137255 0.41568628]\n",
      "   ...\n",
      "   [0.00392157 0.3137255  0.65490198]\n",
      "   [0.         0.26666668 0.59607846]\n",
      "   [0.         0.30588236 0.64705884]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.35294119 0.30980393 0.51372552]\n",
      "   [0.30588236 0.23529412 0.45490196]\n",
      "   ...\n",
      "   [0.00392157 0.26274511 0.6156863 ]\n",
      "   [0.01960784 0.25882354 0.61176473]\n",
      "   [0.00784314 0.27450982 0.63137257]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.24705882 0.28235295 0.39215687]\n",
      "   [0.30980393 0.35294119 0.52941179]\n",
      "   ...\n",
      "   [0.02745098 0.25882354 0.61960787]\n",
      "   [0.03921569 0.24705882 0.60000002]\n",
      "   [0.03137255 0.25490198 0.60784316]]]\n",
      "\n",
      "\n",
      " [[[0.30588236 0.29803923 0.30980393]\n",
      "   [0.40000001 0.40000001 0.39607844]\n",
      "   [0.40000001 0.40000001 0.40000001]\n",
      "   ...\n",
      "   [0.81568629 0.98039216 1.        ]\n",
      "   [0.73333335 0.92941177 1.        ]\n",
      "   [0.60784316 0.8509804  0.98823529]]\n",
      "\n",
      "  [[0.58039218 0.5411765  0.60784316]\n",
      "   [0.64313728 0.63137257 0.68627453]\n",
      "   [0.59215689 0.58823532 0.627451  ]\n",
      "   ...\n",
      "   [0.64313728 0.80784315 0.94509804]\n",
      "   [0.78431374 0.9254902  0.98039216]\n",
      "   [0.67843139 0.89019608 0.99215686]]\n",
      "\n",
      "  [[0.4509804  0.39215687 0.47058824]\n",
      "   [0.44705883 0.40000001 0.47843137]\n",
      "   [0.42745098 0.38431373 0.44313726]\n",
      "   ...\n",
      "   [0.03137255 0.20392157 0.3764706 ]\n",
      "   [0.45882353 0.63529414 0.7764706 ]\n",
      "   [0.74901962 0.9254902  1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.1882353  0.26274511 0.43529412]\n",
      "   [0.16862746 0.22745098 0.38431373]\n",
      "   ...\n",
      "   [0.16078432 0.17647059 0.10980392]\n",
      "   [0.1254902  0.12941177 0.06666667]\n",
      "   [0.15294118 0.15686275 0.09411765]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.21176471 0.32549021 0.49803922]\n",
      "   [0.13725491 0.24313726 0.40000001]\n",
      "   ...\n",
      "   [0.16470589 0.17254902 0.12941177]\n",
      "   [0.17647059 0.17254902 0.1254902 ]\n",
      "   [0.18039216 0.16862746 0.12156863]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.15686275 0.26666668 0.36862746]\n",
      "   [0.18039216 0.32549021 0.4627451 ]\n",
      "   ...\n",
      "   [0.19215687 0.18431373 0.12941177]\n",
      "   [0.18039216 0.16470589 0.1254902 ]\n",
      "   [0.18039216 0.16470589 0.12156863]]]\n",
      "\n",
      "\n",
      " [[[0.21568628 0.22352941 0.30980393]\n",
      "   [0.30588236 0.35294119 0.39607844]\n",
      "   [0.28235295 0.38431373 0.40000001]\n",
      "   ...\n",
      "   [0.66274512 0.67450982 0.64705884]\n",
      "   [0.58039218 0.58039218 0.53725493]\n",
      "   [0.47450981 0.47843137 0.41960785]]\n",
      "\n",
      "  [[0.42745098 0.43529412 0.62352943]\n",
      "   [0.47058824 0.54901963 0.67843139]\n",
      "   [0.40000001 0.56862748 0.63529414]\n",
      "   ...\n",
      "   [0.52549022 0.57647061 0.57647061]\n",
      "   [0.66666669 0.6901961  0.66274512]\n",
      "   [0.54509807 0.54901963 0.50588238]]\n",
      "\n",
      "  [[0.39607844 0.41960785 0.57254905]\n",
      "   [0.3137255  0.38431373 0.50980395]\n",
      "   [0.27450982 0.38431373 0.4627451 ]\n",
      "   ...\n",
      "   [0.01960784 0.08627451 0.11764706]\n",
      "   [0.32549021 0.36078432 0.35294119]\n",
      "   [0.53725493 0.5411765  0.50588238]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.22745098 0.29411766 0.27843139]\n",
      "   [0.23529412 0.26274511 0.25098041]\n",
      "   ...\n",
      "   [0.12941177 0.27058825 0.20784314]\n",
      "   [0.07843138 0.21960784 0.16078432]\n",
      "   [0.11764706 0.23921569 0.20392157]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.21176471 0.34901962 0.34901962]\n",
      "   [0.16470589 0.28627452 0.29411766]\n",
      "   ...\n",
      "   [0.18039216 0.24705882 0.21568628]\n",
      "   [0.2        0.25098041 0.23529412]\n",
      "   [0.19607843 0.25490198 0.21960784]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.18039216 0.29019609 0.29803923]\n",
      "   [0.23137255 0.38039216 0.38039216]\n",
      "   ...\n",
      "   [0.22352941 0.26274511 0.23921569]\n",
      "   [0.22745098 0.25098041 0.23921569]\n",
      "   [0.22745098 0.25882354 0.23529412]]]].\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 204, in generator_py_func\n",
      "    flattened_values = nest.flatten_up_to(output_types, values)\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/data/util/nest.py\", line 237, in flatten_up_to\n",
      "    return nest_util.flatten_up_to(\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/util/nest_util.py\", line 1541, in flatten_up_to\n",
      "    return _tf_data_flatten_up_to(shallow_tree, input_tree)\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/util/nest_util.py\", line 1570, in _tf_data_flatten_up_to\n",
      "    _tf_data_assert_shallow_structure(shallow_tree, input_tree)\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/util/nest_util.py\", line 1414, in _tf_data_assert_shallow_structure\n",
      "    raise TypeError(\n",
      "\n",
      "TypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: 'ndarray'.\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 206, in generator_py_func\n",
      "    raise TypeError(\n",
      "\n",
      "TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float64,), but the yielded element was [[[[0.20392157 0.17254902 0.18431373]\n",
      "   [0.24705882 0.23529412 0.23921569]\n",
      "   [0.20392157 0.20784314 0.20392157]\n",
      "   ...\n",
      "   [0.99607843 0.98823529 0.98039216]\n",
      "   [0.98823529 0.98039216 0.97254902]\n",
      "   [0.96862745 0.95294118 0.93725491]]\n",
      "\n",
      "  [[0.38431373 0.32549021 0.34509805]\n",
      "   [0.34509805 0.33725491 0.33725491]\n",
      "   [0.34117648 0.35686275 0.34901962]\n",
      "   ...\n",
      "   [0.78823531 0.78039217 0.7764706 ]\n",
      "   [0.94117647 0.94117647 0.93725491]\n",
      "   [0.98039216 0.97647059 0.96862745]]\n",
      "\n",
      "  [[0.3137255  0.25490198 0.27450982]\n",
      "   [0.15686275 0.15686275 0.15686275]\n",
      "   [0.26666668 0.28235295 0.27058825]\n",
      "   ...\n",
      "   [0.20392157 0.18431373 0.20392157]\n",
      "   [0.70980394 0.7019608  0.70980394]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.31764707 0.34509805 0.42352942]\n",
      "   [0.23529412 0.25882354 0.33725491]\n",
      "   ...\n",
      "   [0.31764707 0.31764707 0.35686275]\n",
      "   [0.27450982 0.29411766 0.30980393]\n",
      "   [0.3137255  0.32941177 0.36470589]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.34509805 0.37254903 0.42745098]\n",
      "   [0.25490198 0.28235295 0.32549021]\n",
      "   ...\n",
      "   [0.31764707 0.32941177 0.35294119]\n",
      "   [0.29803923 0.3137255  0.33725491]\n",
      "   [0.30588236 0.30980393 0.34117648]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.25490198 0.27843139 0.3137255 ]\n",
      "   [0.29411766 0.32156864 0.36470589]\n",
      "   ...\n",
      "   [0.32156864 0.33333334 0.36470589]\n",
      "   [0.29019609 0.30980393 0.35294119]\n",
      "   [0.28627452 0.3019608  0.34509805]]]\n",
      "\n",
      "\n",
      " [[[0.10980392 0.13725491 0.18039216]\n",
      "   [0.15294118 0.18431373 0.23529412]\n",
      "   [0.15686275 0.16470589 0.20784314]\n",
      "   ...\n",
      "   [0.96862745 0.97647059 0.98823529]\n",
      "   [0.90980393 0.9137255  0.92941177]\n",
      "   [0.84705883 0.8509804  0.87450981]]\n",
      "\n",
      "  [[0.2        0.24705882 0.32941177]\n",
      "   [0.23529412 0.28235295 0.36470589]\n",
      "   [0.23921569 0.25490198 0.31764707]\n",
      "   ...\n",
      "   [0.78431374 0.7764706  0.78823531]\n",
      "   [0.94901961 0.94901961 0.95294118]\n",
      "   [0.90980393 0.90588236 0.92156863]]\n",
      "\n",
      "  [[0.15294118 0.18431373 0.24705882]\n",
      "   [0.14901961 0.17254902 0.23529412]\n",
      "   [0.16078432 0.16078432 0.20784314]\n",
      "   ...\n",
      "   [0.13725491 0.10980392 0.13333334]\n",
      "   [0.66274512 0.65098041 0.66666669]\n",
      "   [0.94117647 0.94509804 0.95686275]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.34901962 0.43529412 0.50588238]\n",
      "   [0.32549021 0.38039216 0.45882353]\n",
      "   ...\n",
      "   [0.38039216 0.36078432 0.43137255]\n",
      "   [0.33725491 0.3019608  0.3882353 ]\n",
      "   [0.38039216 0.32941177 0.42352942]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.38039216 0.44705883 0.52156866]\n",
      "   [0.34509805 0.41176471 0.48235294]\n",
      "   ...\n",
      "   [0.38039216 0.33333334 0.42352942]\n",
      "   [0.39607844 0.33725491 0.43137255]\n",
      "   [0.39607844 0.33725491 0.42745098]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.25882354 0.29803923 0.34509805]\n",
      "   [0.32549021 0.38431373 0.44705883]\n",
      "   ...\n",
      "   [0.40392157 0.34117648 0.44313726]\n",
      "   [0.39215687 0.32941177 0.42352942]\n",
      "   [0.3882353  0.32941177 0.41960785]]]\n",
      "\n",
      "\n",
      " [[[0.23137255 0.28235295 0.3019608 ]\n",
      "   [0.3137255  0.36862746 0.40000001]\n",
      "   [0.31764707 0.36470589 0.40000001]\n",
      "   ...\n",
      "   [0.47450981 0.59607846 0.70588237]\n",
      "   [0.36862746 0.49019608 0.60784316]\n",
      "   [0.25490198 0.38039216 0.49019608]]\n",
      "\n",
      "  [[0.41568628 0.50588238 0.56470591]\n",
      "   [0.48627451 0.57254905 0.64313728]\n",
      "   [0.48235294 0.56078434 0.63137257]\n",
      "   ...\n",
      "   [0.40000001 0.48235294 0.59607846]\n",
      "   [0.45882353 0.54901963 0.66666669]\n",
      "   [0.32941177 0.43529412 0.53725493]]\n",
      "\n",
      "  [[0.32156864 0.38039216 0.43921569]\n",
      "   [0.3019608  0.36470589 0.42352942]\n",
      "   [0.29803923 0.35294119 0.41568628]\n",
      "   ...\n",
      "   [0.00784314 0.03137255 0.14509805]\n",
      "   [0.24313726 0.29803923 0.42352942]\n",
      "   [0.3764706  0.46666667 0.58039218]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.49019608 0.35686275 0.48235294]\n",
      "   [0.3882353  0.27450982 0.3764706 ]\n",
      "   ...\n",
      "   [0.11764706 0.19607843 0.24313726]\n",
      "   [0.07450981 0.14509805 0.19215687]\n",
      "   [0.10588235 0.16470589 0.22352941]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.50588238 0.38039216 0.49803922]\n",
      "   [0.36862746 0.25098041 0.36078432]\n",
      "   ...\n",
      "   [0.09411765 0.15686275 0.21960784]\n",
      "   [0.10196079 0.16078432 0.22745098]\n",
      "   [0.10588235 0.16862746 0.22352941]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.36078432 0.29411766 0.36470589]\n",
      "   [0.43921569 0.33725491 0.44313726]\n",
      "   ...\n",
      "   [0.10980392 0.17647059 0.23921569]\n",
      "   [0.10980392 0.17647059 0.23921569]\n",
      "   [0.10980392 0.17647059 0.23921569]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.21568628 0.17254902 0.2       ]\n",
      "   [0.32156864 0.23529412 0.24313726]\n",
      "   [0.36862746 0.22352941 0.24705882]\n",
      "   ...\n",
      "   [0.85882354 0.78823531 0.8392157 ]\n",
      "   [0.79215688 0.69803923 0.73725492]\n",
      "   [0.6901961  0.57647061 0.6156863 ]]\n",
      "\n",
      "  [[0.39215687 0.32549021 0.36862746]\n",
      "   [0.47450981 0.35294119 0.34901962]\n",
      "   [0.65882355 0.35686275 0.40000001]\n",
      "   ...\n",
      "   [0.64705884 0.63529414 0.71372551]\n",
      "   [0.80784315 0.76078433 0.81960785]\n",
      "   [0.73333335 0.63921571 0.68235296]]\n",
      "\n",
      "  [[0.29411766 0.24705882 0.27843139]\n",
      "   [0.28235295 0.21960784 0.20784314]\n",
      "   [0.47450981 0.23137255 0.25098041]\n",
      "   ...\n",
      "   [0.05882353 0.09803922 0.23137255]\n",
      "   [0.45490196 0.43921569 0.53333336]\n",
      "   [0.73333335 0.66666669 0.73725492]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.36470589 0.27843139 0.47058824]\n",
      "   [0.33725491 0.23137255 0.41568628]\n",
      "   ...\n",
      "   [0.00392157 0.3137255  0.65490198]\n",
      "   [0.         0.26666668 0.59607846]\n",
      "   [0.         0.30588236 0.64705884]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.35294119 0.30980393 0.51372552]\n",
      "   [0.30588236 0.23529412 0.45490196]\n",
      "   ...\n",
      "   [0.00392157 0.26274511 0.6156863 ]\n",
      "   [0.01960784 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.25882354 0.61176473]\n",
      "   [0.00784314 0.27450982 0.63137257]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.24705882 0.28235295 0.39215687]\n",
      "   [0.30980393 0.35294119 0.52941179]\n",
      "   ...\n",
      "   [0.02745098 0.25882354 0.61960787]\n",
      "   [0.03921569 0.24705882 0.60000002]\n",
      "   [0.03137255 0.25490198 0.60784316]]]\n",
      "\n",
      "\n",
      " [[[0.30588236 0.29803923 0.30980393]\n",
      "   [0.40000001 0.40000001 0.39607844]\n",
      "   [0.40000001 0.40000001 0.40000001]\n",
      "   ...\n",
      "   [0.81568629 0.98039216 1.        ]\n",
      "   [0.73333335 0.92941177 1.        ]\n",
      "   [0.60784316 0.8509804  0.98823529]]\n",
      "\n",
      "  [[0.58039218 0.5411765  0.60784316]\n",
      "   [0.64313728 0.63137257 0.68627453]\n",
      "   [0.59215689 0.58823532 0.627451  ]\n",
      "   ...\n",
      "   [0.64313728 0.80784315 0.94509804]\n",
      "   [0.78431374 0.9254902  0.98039216]\n",
      "   [0.67843139 0.89019608 0.99215686]]\n",
      "\n",
      "  [[0.4509804  0.39215687 0.47058824]\n",
      "   [0.44705883 0.40000001 0.47843137]\n",
      "   [0.42745098 0.38431373 0.44313726]\n",
      "   ...\n",
      "   [0.03137255 0.20392157 0.3764706 ]\n",
      "   [0.45882353 0.63529414 0.7764706 ]\n",
      "   [0.74901962 0.9254902  1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.1882353  0.26274511 0.43529412]\n",
      "   [0.16862746 0.22745098 0.38431373]\n",
      "   ...\n",
      "   [0.16078432 0.17647059 0.10980392]\n",
      "   [0.1254902  0.12941177 0.06666667]\n",
      "   [0.15294118 0.15686275 0.09411765]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.21176471 0.32549021 0.49803922]\n",
      "   [0.13725491 0.24313726 0.40000001]\n",
      "   ...\n",
      "   [0.16470589 0.17254902 0.12941177]\n",
      "   [0.17647059 0.17254902 0.1254902 ]\n",
      "   [0.18039216 0.16862746 0.12156863]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.15686275 0.26666668 0.36862746]\n",
      "   [0.18039216 0.32549021 0.4627451 ]\n",
      "   ...\n",
      "   [0.19215687 0.18431373 0.12941177]\n",
      "   [0.18039216 0.16470589 0.1254902 ]\n",
      "   [0.18039216 0.16470589 0.12156863]]]\n",
      "\n",
      "\n",
      " [[[0.21568628 0.22352941 0.30980393]\n",
      "   [0.30588236 0.35294119 0.39607844]\n",
      "   [0.28235295 0.38431373 0.40000001]\n",
      "   ...\n",
      "   [0.66274512 0.67450982 0.64705884]\n",
      "   [0.58039218 0.58039218 0.53725493]\n",
      "   [0.47450981 0.47843137 0.41960785]]\n",
      "\n",
      "  [[0.42745098 0.43529412 0.62352943]\n",
      "   [0.47058824 0.54901963 0.67843139]\n",
      "   [0.40000001 0.56862748 0.63529414]\n",
      "   ...\n",
      "   [0.52549022 0.57647061 0.57647061]\n",
      "   [0.66666669 0.6901961  0.66274512]\n",
      "   [0.54509807 0.54901963 0.50588238]]\n",
      "\n",
      "  [[0.39607844 0.41960785 0.57254905]\n",
      "   [0.3137255  0.38431373 0.50980395]\n",
      "   [0.27450982 0.38431373 0.4627451 ]\n",
      "   ...\n",
      "   [0.01960784 0.08627451 0.11764706]\n",
      "   [0.32549021 0.36078432 0.35294119]\n",
      "   [0.53725493 0.5411765  0.50588238]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.22745098 0.29411766 0.27843139]\n",
      "   [0.23529412 0.26274511 0.25098041]\n",
      "   ...\n",
      "   [0.12941177 0.27058825 0.20784314]\n",
      "   [0.07843138 0.21960784 0.16078432]\n",
      "   [0.11764706 0.23921569 0.20392157]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.21176471 0.34901962 0.34901962]\n",
      "   [0.16470589 0.28627452 0.29411766]\n",
      "   ...\n",
      "   [0.18039216 0.24705882 0.21568628]\n",
      "   [0.2        0.25098041 0.23529412]\n",
      "   [0.19607843 0.25490198 0.21960784]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.18039216 0.29019609 0.29803923]\n",
      "   [0.23137255 0.38039216 0.38039216]\n",
      "   ...\n",
      "   [0.22352941 0.26274511 0.23921569]\n",
      "   [0.22745098 0.25098041 0.23921569]\n",
      "   [0.22745098 0.25882354 0.23529412]]]].\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "\t [[IteratorGetNext]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "2024-11-01 22:38:29.439105: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 6388348759201605071\n",
      "2024-11-01 22:38:29.901197: W tensorflow/core/framework/op_kernel.cc:1828] INVALID_ARGUMENT: TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float64,), but the yielded element was [[[[0.18431373 0.12941177 0.14117648]\n",
      "   [0.25882354 0.2        0.20392157]\n",
      "   [0.34901962 0.23529412 0.23921569]\n",
      "   ...\n",
      "   [0.6156863  0.80784315 0.90588236]\n",
      "   [0.49803922 0.69411767 0.81176472]\n",
      "   [0.39607844 0.59215689 0.72156864]]\n",
      "\n",
      "  [[0.36078432 0.25098041 0.27843139]\n",
      "   [0.33333334 0.26666668 0.27058825]\n",
      "   [0.58431375 0.35294119 0.35686275]\n",
      "   ...\n",
      "   [0.49411765 0.65882355 0.74901962]\n",
      "   [0.59607846 0.77254903 0.88235295]\n",
      "   [0.49411765 0.68235296 0.80000001]]\n",
      "\n",
      "  [[0.30588236 0.19215687 0.21176471]\n",
      "   [0.22745098 0.2        0.2       ]\n",
      "   [0.38039216 0.27843139 0.24313726]\n",
      "   ...\n",
      "   [0.03137255 0.14509805 0.23529412]\n",
      "   [0.32549021 0.47058824 0.56078434]\n",
      "   [0.48627451 0.66666669 0.78039217]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.26666668 0.20784314 0.2       ]\n",
      "   [0.23529412 0.1882353  0.19215687]\n",
      "   ...\n",
      "   [0.22745098 0.25098041 0.3019608 ]\n",
      "   [0.19215687 0.21176471 0.25882354]\n",
      "   [0.21960784 0.23921569 0.29411766]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.27843139 0.23529412 0.22352941]\n",
      "   [0.21176471 0.17647059 0.16862746]\n",
      "   ...\n",
      "   [0.21568628 0.23529412 0.27450982]\n",
      "   [0.21960784 0.22745098 0.28627452]\n",
      "   [0.22745098 0.23529412 0.28235295]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.22745098 0.21568628 0.20392157]\n",
      "   [0.28627452 0.27843139 0.25882354]\n",
      "   ...\n",
      "   [0.23137255 0.23529412 0.29019609]\n",
      "   [0.23529412 0.23137255 0.29019609]\n",
      "   [0.23529412 0.23137255 0.28235295]]]\n",
      "\n",
      "\n",
      " [[[0.21568628 0.17254902 0.2       ]\n",
      "   [0.32156864 0.23529412 0.24313726]\n",
      "   [0.36862746 0.22352941 0.24705882]\n",
      "   ...\n",
      "   [0.85882354 0.78823531 0.8392157 ]\n",
      "   [0.79215688 0.69803923 0.73725492]\n",
      "   [0.6901961  0.57647061 0.6156863 ]]\n",
      "\n",
      "  [[0.39215687 0.32549021 0.36862746]\n",
      "   [0.47450981 0.35294119 0.34901962]\n",
      "   [0.65882355 0.35686275 0.40000001]\n",
      "   ...\n",
      "   [0.64705884 0.63529414 0.71372551]\n",
      "   [0.80784315 0.76078433 0.81960785]\n",
      "   [0.73333335 0.63921571 0.68235296]]\n",
      "\n",
      "  [[0.29411766 0.24705882 0.27843139]\n",
      "   [0.28235295 0.21960784 0.20784314]\n",
      "   [0.47450981 0.23137255 0.25098041]\n",
      "   ...\n",
      "   [0.05882353 0.09803922 0.23137255]\n",
      "   [0.45490196 0.43921569 0.53333336]\n",
      "   [0.73333335 0.66666669 0.73725492]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.36470589 0.27843139 0.47058824]\n",
      "   [0.33725491 0.23137255 0.41568628]\n",
      "   ...\n",
      "   [0.00392157 0.3137255  0.65490198]\n",
      "   [0.         0.26666668 0.59607846]\n",
      "   [0.         0.30588236 0.64705884]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.35294119 0.30980393 0.51372552]\n",
      "   [0.30588236 0.23529412 0.45490196]\n",
      "   ...\n",
      "   [0.00392157 0.26274511 0.6156863 ]\n",
      "   [0.01960784 0.25882354 0.61176473]\n",
      "   [0.00784314 0.27450982 0.63137257]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.24705882 0.28235295 0.39215687]\n",
      "   [0.30980393 0.35294119 0.52941179]\n",
      "   ...\n",
      "   [0.02745098 0.25882354 0.61960787]\n",
      "   [0.03921569 0.24705882 0.60000002]\n",
      "   [0.03137255 0.25490198 0.60784316]]]\n",
      "\n",
      "\n",
      " [[[0.30588236 0.29803923 0.30980393]\n",
      "   [0.40000001 0.40000001 0.39607844]\n",
      "   [0.40000001 0.40000001 0.40000001]\n",
      "   ...\n",
      "   [0.81568629 0.98039216 1.        ]\n",
      "   [0.73333335 0.92941177 1.        ]\n",
      "   [0.60784316 0.8509804  0.98823529]]\n",
      "\n",
      "  [[0.58039218 0.5411765  0.60784316]\n",
      "   [0.64313728 0.63137257 0.68627453]\n",
      "   [0.59215689 0.58823532 0.627451  ]\n",
      "   ...\n",
      "   [0.64313728 0.80784315 0.94509804]\n",
      "   [0.78431374 0.9254902  0.98039216]\n",
      "   [0.67843139 0.89019608 0.99215686]]\n",
      "\n",
      "  [[0.4509804  0.39215687 0.47058824]\n",
      "   [0.44705883 0.40000001 0.47843137]\n",
      "   [0.42745098 0.38431373 0.44313726]\n",
      "   ...\n",
      "   [0.03137255 0.20392157 0.3764706 ]\n",
      "   [0.45882353 0.63529414 0.7764706 ]\n",
      "   [0.74901962 0.9254902  1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.1882353  0.26274511 0.43529412]\n",
      "   [0.16862746 0.22745098 0.38431373]\n",
      "   ...\n",
      "   [0.16078432 0.17647059 0.10980392]\n",
      "   [0.1254902  0.12941177 0.06666667]\n",
      "   [0.15294118 0.15686275 0.09411765]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.21176471 0.32549021 0.49803922]\n",
      "   [0.13725491 0.24313726 0.40000001]\n",
      "   ...\n",
      "   [0.16470589 0.17254902 0.12941177]\n",
      "   [0.17647059 0.17254902 0.1254902 ]\n",
      "   [0.18039216 0.16862746 0.12156863]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.15686275 0.26666668 0.36862746]\n",
      "   [0.18039216 0.32549021 0.4627451 ]\n",
      "   ...\n",
      "   [0.19215687 0.18431373 0.12941177]\n",
      "   [0.18039216 0.16470589 0.1254902 ]\n",
      "   [0.18039216 0.16470589 0.12156863]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.31764707 0.32156864 0.32549021]\n",
      "   [0.39215687 0.39215687 0.3882353 ]\n",
      "   [0.39607844 0.39607844 0.39215687]\n",
      "   ...\n",
      "   [0.81960785 0.89803922 0.93725491]\n",
      "   [0.74509805 0.84313726 0.89411765]\n",
      "   [0.60392159 0.72941178 0.79607844]]\n",
      "\n",
      "  [[0.61176473 0.63137257 0.67058825]\n",
      "   [0.75294119 0.76862746 0.80392158]\n",
      "   [0.6901961  0.70588237 0.74117649]\n",
      "   ...\n",
      "   [0.7019608  0.74117649 0.79607844]\n",
      "   [0.81568629 0.89019608 0.9137255 ]\n",
      "   [0.7019608  0.80784315 0.85882354]]\n",
      "\n",
      "  [[0.5529412  0.57647061 0.62352943]\n",
      "   [0.48235294 0.50588238 0.5529412 ]\n",
      "   [0.52941179 0.55686277 0.60392159]\n",
      "   ...\n",
      "   [0.13333334 0.19215687 0.25882354]\n",
      "   [0.50980395 0.60000002 0.64313728]\n",
      "   [0.78039217 0.87843138 0.92156863]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.49019608 0.48235294 0.50588238]\n",
      "   [0.44313726 0.43529412 0.47058824]\n",
      "   ...\n",
      "   [0.24705882 0.27843139 0.3019608 ]\n",
      "   [0.17254902 0.2        0.22352941]\n",
      "   [0.17647059 0.2        0.24705882]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.52941179 0.52549022 0.53333336]\n",
      "   [0.49803922 0.49019608 0.51764709]\n",
      "   ...\n",
      "   [0.23529412 0.27058825 0.30588236]\n",
      "   [0.25098041 0.27450982 0.30980393]\n",
      "   [0.20392157 0.23921569 0.26274511]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.3882353  0.3882353  0.39607844]\n",
      "   [0.52941179 0.52549022 0.54509807]\n",
      "   ...\n",
      "   [0.22745098 0.25490198 0.28235295]\n",
      "   [0.22745098 0.23921569 0.25882354]\n",
      "   [0.21176471 0.23529412 0.25490198]]]\n",
      "\n",
      "\n",
      " [[[0.28235295 0.27843139 0.29019609]\n",
      "   [0.39607844 0.39215687 0.40000001]\n",
      "   [0.37254903 0.34901962 0.37254903]\n",
      "   ...\n",
      "   [0.80000001 0.86666667 0.88235295]\n",
      "   [0.6156863  0.7764706  0.78431374]\n",
      "   [0.3882353  0.57254905 0.59607846]]\n",
      "\n",
      "  [[0.49803922 0.49411765 0.51764709]\n",
      "   [0.58431375 0.58823532 0.60784316]\n",
      "   [0.5529412  0.50980395 0.5529412 ]\n",
      "   ...\n",
      "   [0.62352943 0.60000002 0.63529414]\n",
      "   [0.75686276 0.82745099 0.84313726]\n",
      "   [0.52941179 0.65490198 0.68235296]]\n",
      "\n",
      "  [[0.33725491 0.34117648 0.36078432]\n",
      "   [0.40000001 0.40000001 0.41568628]\n",
      "   [0.41176471 0.38431373 0.40784314]\n",
      "   ...\n",
      "   [0.11764706 0.11764706 0.14901961]\n",
      "   [0.46666667 0.5529412  0.57647061]\n",
      "   [0.57254905 0.65882355 0.71764708]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.47058824 0.49803922 0.52941179]\n",
      "   [0.38039216 0.45490196 0.61960787]\n",
      "   ...\n",
      "   [0.14117648 0.16078432 0.16862746]\n",
      "   [0.10196079 0.10980392 0.1254902 ]\n",
      "   [0.1254902  0.14117648 0.16078432]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.50196081 0.52156866 0.54509807]\n",
      "   [0.40784314 0.49803922 0.64705884]\n",
      "   ...\n",
      "   [0.1254902  0.14509805 0.16078432]\n",
      "   [0.13333334 0.15294118 0.16862746]\n",
      "   [0.13725491 0.15686275 0.17254902]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.35294119 0.3882353  0.44313726]\n",
      "   [0.41960785 0.50196081 0.63529414]\n",
      "   ...\n",
      "   [0.13333334 0.15294118 0.16862746]\n",
      "   [0.11764706 0.13725491 0.15294118]\n",
      "   [0.12156863 0.14117648 0.15686275]]]\n",
      "\n",
      "\n",
      " [[[0.20392157 0.25490198 0.28627452]\n",
      "   [0.28235295 0.35294119 0.39215687]\n",
      "   [0.26666668 0.33725491 0.38039216]\n",
      "   ...\n",
      "   [0.43921569 0.52156866 0.60392159]\n",
      "   [0.36470589 0.44705883 0.52941179]\n",
      "   [0.27450982 0.35686275 0.43921569]]\n",
      "\n",
      "  [[0.3764706  0.4627451  0.52549022]\n",
      "   [0.42745098 0.53725493 0.60392159]\n",
      "   [0.42745098 0.53333336 0.60000002]\n",
      "   ...\n",
      "   [0.36078432 0.4509804  0.52941179]\n",
      "   [0.41176471 0.49411765 0.57647061]\n",
      "   [0.32549021 0.40784314 0.49019608]]\n",
      "\n",
      "  [[0.30980393 0.36078432 0.40784314]\n",
      "   [0.28235295 0.33333334 0.36862746]\n",
      "   [0.30980393 0.35686275 0.39607844]\n",
      "   ...\n",
      "   [0.01176471 0.09411765 0.17647059]\n",
      "   [0.23137255 0.31764707 0.40392157]\n",
      "   [0.30588236 0.39215687 0.47450981]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.5411765  0.54901963 0.5529412 ]\n",
      "   [0.66666669 0.70980394 0.74901962]\n",
      "   ...\n",
      "   [0.36078432 0.39607844 0.42352942]\n",
      "   [0.31764707 0.34509805 0.3764706 ]\n",
      "   [0.35294119 0.3764706  0.40000001]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.56078434 0.56470591 0.57254905]\n",
      "   [0.72156864 0.7647059  0.81568629]\n",
      "   ...\n",
      "   [0.34509805 0.37254903 0.39607844]\n",
      "   [0.35686275 0.36862746 0.39215687]\n",
      "   [0.36078432 0.36862746 0.40000001]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.41568628 0.42745098 0.44705883]\n",
      "   [0.6156863  0.65098041 0.69803923]\n",
      "   ...\n",
      "   [0.35686275 0.36470589 0.38431373]\n",
      "   [0.34901962 0.34901962 0.36470589]\n",
      "   [0.34901962 0.35294119 0.37254903]]]].\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 204, in generator_py_func\n",
      "    flattened_values = nest.flatten_up_to(output_types, values)\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/data/util/nest.py\", line 237, in flatten_up_to\n",
      "    return nest_util.flatten_up_to(\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/util/nest_util.py\", line 1541, in flatten_up_to\n",
      "    return _tf_data_flatten_up_to(shallow_tree, input_tree)\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/util/nest_util.py\", line 1570, in _tf_data_flatten_up_to\n",
      "    _tf_data_assert_shallow_structure(shallow_tree, input_tree)\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/util/nest_util.py\", line 1414, in _tf_data_assert_shallow_structure\n",
      "    raise TypeError(\n",
      "\n",
      "TypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: 'ndarray'.\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/usr/lib/python3/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 206, in generator_py_func\n",
      "    raise TypeError(\n",
      "\n",
      "TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float64,), but the yielded element was [[[[0.18431373 0.12941177 0.14117648]\n",
      "   [0.25882354 0.2        0.20392157]\n",
      "   [0.34901962 0.23529412 0.23921569]\n",
      "   ...\n",
      "   [0.6156863  0.80784315 0.90588236]\n",
      "   [0.49803922 0.69411767 0.81176472]\n",
      "   [0.39607844 0.59215689 0.72156864]]\n",
      "\n",
      "  [[0.36078432 0.25098041 0.27843139]\n",
      "   [0.33333334 0.26666668 0.27058825]\n",
      "   [0.58431375 0.35294119 0.35686275]\n",
      "   ...\n",
      "   [0.49411765 0.65882355 0.74901962]\n",
      "   [0.59607846 0.77254903 0.88235295]\n",
      "   [0.49411765 0.68235296 0.80000001]]\n",
      "\n",
      "  [[0.30588236 0.19215687 0.21176471]\n",
      "   [0.22745098 0.2        0.2       ]\n",
      "   [0.38039216 0.27843139 0.24313726]\n",
      "   ...\n",
      "   [0.03137255 0.14509805 0.23529412]\n",
      "   [0.32549021 0.47058824 0.56078434]\n",
      "   [0.48627451 0.66666669 0.78039217]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.26666668 0.20784314 0.2       ]\n",
      "   [0.23529412 0.1882353  0.19215687]\n",
      "   ...\n",
      "   [0.22745098 0.25098041 0.3019608 ]\n",
      "   [0.19215687 0.21176471 0.25882354]\n",
      "   [0.21960784 0.23921569 0.29411766]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.27843139 0.23529412 0.22352941]\n",
      "   [0.21176471 0.17647059 0.16862746]\n",
      "   ...\n",
      "   [0.21568628 0.23529412 0.27450982]\n",
      "   [0.21960784 0.22745098 0.28627452]\n",
      "   [0.22745098 0.23529412 0.28235295]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.22745098 0.21568628 0.20392157]\n",
      "   [0.28627452 0.27843139 0.25882354]\n",
      "   ...\n",
      "   [0.23137255 0.23529412 0.29019609]\n",
      "   [0.23529412 0.23137255 0.29019609]\n",
      "   [0.23529412 0.23137255 0.28235295]]]\n",
      "\n",
      "\n",
      " [[[0.21568628 0.17254902 0.2       ]\n",
      "   [0.32156864 0.23529412 0.24313726]\n",
      "   [0.36862746 0.22352941 0.24705882]\n",
      "   ...\n",
      "   [0.85882354 0.78823531 0.8392157 ]\n",
      "   [0.79215688 0.69803923 0.73725492]\n",
      "   [0.6901961  0.57647061 0.6156863 ]]\n",
      "\n",
      "  [[0.39215687 0.32549021 0.36862746]\n",
      "   [0.47450981 0.35294119 0.34901962]\n",
      "   [0.65882355 0.35686275 0.40000001]\n",
      "   ...\n",
      "   [0.64705884 0.63529414 0.71372551]\n",
      "   [0.80784315 0.76078433 0.81960785]\n",
      "   [0.73333335 0.63921571 0.68235296]]\n",
      "\n",
      "  [[0.29411766 0.24705882 0.27843139]\n",
      "   [0.28235295 0.21960784 0.20784314]\n",
      "   [0.47450981 0.23137255 0.25098041]\n",
      "   ...\n",
      "   [0.05882353 0.09803922 0.23137255]\n",
      "   [0.45490196 0.43921569 0.53333336]\n",
      "   [0.73333335 0.66666669 0.73725492]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.36470589 0.27843139 0.47058824]\n",
      "   [0.33725491 0.23137255 0.41568628]\n",
      "   ...\n",
      "   [0.00392157 0.3137255  0.65490198]\n",
      "   [0.         0.26666668 0.59607846]\n",
      "   [0.         0.30588236 0.64705884]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.35294119 0.30980393 0.51372552]\n",
      "   [0.30588236 0.23529412 0.45490196]\n",
      "   ...\n",
      "   [0.00392157 0.26274511 0.6156863 ]\n",
      "   [0.01960784 0.25882354 0.61176473]\n",
      "   [0.00784314 0.27450982 0.63137257]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.24705882 0.28235295 0.39215687]\n",
      "   [0.30980393 0.35294119 0.52941179]\n",
      "   ...\n",
      "   [0.02745098 0.25882354 0.61960787]\n",
      "   [0.03921569 0.24705882 0.60000002]\n",
      "   [0.03137255 0.25490198 0.60784316]]]\n",
      "\n",
      "\n",
      " [[[0.30588236 0.29803923 0.30980393]\n",
      "   [0.40000001 0.40000001 0.39607844]\n",
      "   [0.40000001 0.40000001 0.40000001]\n",
      "   ...\n",
      "   [0.81568629 0.98039216 1.        ]\n",
      "   [0.73333335 0.92941177 1.        ]\n",
      "   [0.60784316 0.8509804  0.98823529]]\n",
      "\n",
      "  [[0.58039218 0.5411765  0.60784316]\n",
      "   [0.64313728 0.63137257 0.68627453]\n",
      "   [0.59215689 0.58823532 0.627451  ]\n",
      "   ...\n",
      "   [0.64313728 0.80784315 0.94509804]\n",
      "   [0.78431374 0.9254902  0.98039216]\n",
      "   [0.67843139 0.89019608 0.99215686]]\n",
      "\n",
      "  [[0.4509804  0.39215687 0.47058824]\n",
      "   [0.44705883 0.40000001 0.47843137]\n",
      "   [0.42745098 0.38431373 0.44313726]\n",
      "   ...\n",
      "   [0.03137255 0.20392157 0.3764706 ]\n",
      "   [0.45882353 0.63529414 0.7764706 ]\n",
      "   [0.74901962 0.9254902  1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.1882353  0.26274511 0.43529412]\n",
      "   [0.16862746 0.22745098 0.38431373]\n",
      "   ...\n",
      "   [0.16078432 0.17647059 0.10980392]\n",
      "   [0.1254902  0.12941177 0.06666667]\n",
      "   [0.15294118 0.15686275 0.09411765]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.21176471 0.32549021 0.49803922]\n",
      "   [0.13725491 0.24313726 0.40000001]\n",
      "   ...\n",
      "   [0.16470589 0.17254902 0.12941177]\n",
      "   [0.17647059 0.17254902 0.1254902 ]\n",
      "   [0.18039216 0.16862746 0.12156863]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.15686275 0.26666668 0.36862746]\n",
      "   [0.18039216 0.32549021 0.4627451 ]\n",
      "   ...\n",
      "   [0.19215687 0.18431373 0.12941177]\n",
      "   [0.18039216 0.16470589 0.1254902 ]\n",
      "   [0.18039216 0.16470589 0.12156863]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.31764707 0.32156864 0.32549021]\n",
      "   [0.39215687 0.39215687 0.3882353 ]\n",
      "   [0.39607844 0.39607844 0.39215687]\n",
      "   ...\n",
      "   [0.81960785 0.89803922 0.93725491]\n",
      "   [0.74509805 0.84313726 0.89411765]\n",
      "   [0.60392159 0.72941178 0.79607844]]\n",
      "\n",
      "  [[0.61176473 0.63137257 0.67058825]\n",
      "   [0.75294119 0.76862746 0.80392158]\n",
      "   [0.6901961  0.70588237 0.74117649]\n",
      "   ...\n",
      "   [0.7019608  0.74117649 0.79607844]\n",
      "   [0.81568629 0.89019608 0.9137255 ]\n",
      "   [0.7019608  0.80784315 0.85882354]]\n",
      "\n",
      "  [[0.5529412  0.57647061 0.62352943]\n",
      "   [0.48235294 0.50588238 0.5529412 ]\n",
      "   [0.52941179 0.55686277 0.60392159]\n",
      "   ...\n",
      "   [0.13333334 0.19215687 0.25882354]\n",
      "   [0.50980395 0.60000002 0.64313728]\n",
      "   [0.78039217 0.87843138 0.92156863]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.49019608 0.48235294 0.50588238]\n",
      "   [0.44313726 0.43529412 0.47058824]\n",
      "   ...\n",
      "   [0.24705882 0.27843139 0.3019608 ]\n",
      "   [0.17254902 0.2        0.22352941]\n",
      "   [0.17647059 0.2        0.24705882]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.52941179 0.52549022 0.53333336]\n",
      "   [0.49803922 0.49019608 0.51764709]\n",
      "   ...\n",
      "   [0.23529412 0.27058825 0.30588236]\n",
      "   [0.25098041 0.27450982 0.30980393]\n",
      "   [0.20392157 0.23921569 0.26274511]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.3882353  0.3882353  0.39607844]\n",
      "   [0.52941179 0.52549022 0.54509807]\n",
      "   ...\n",
      "   [0.22745098 0.25490198 0.28235295]\n",
      "   [0.22745098 0.23921569 0.25882354]\n",
      "   [0.21176471 0.23529412 0.25490198]]]\n",
      "\n",
      "\n",
      " [[[0.28235295 0.27843139 0.29019609]\n",
      "   [0.39607844 0.39215687 0.40000001]\n",
      "   [0.37254903 0.34901962 0.37254903]\n",
      "   ...\n",
      "   [0.80000001 0.86666667 0.88235295]\n",
      "   [0.6156863  0.7764706  0.78431374]\n",
      "   [0.3882353  0.57254905 0.59607846]]\n",
      "\n",
      "  [[0.49803922 0.49411765 0.51764709]\n",
      "   [0.58431375 0.58823532 0.60784316]\n",
      "   [0.5529412  0.50980395 0.5529412 ]\n",
      "   ...\n",
      "   [0.62352943 0.60000002 0.63529414]\n",
      "   [0.75686276 0.82745099 0.84313726]\n",
      "   [0.52941179 0.65490198 0.68235296]]\n",
      "\n",
      "  [[0.33725491 0.34117648 0.36078432]\n",
      "   [0.40000001 0.40000001 0.41568628]\n",
      "   [0.41176471 0.38431373 0.40784314]\n",
      "   ...\n",
      "   [0.11764706 0.11764706 0.14901961]\n",
      "   [0.46666667 0.5529412  0.57647061]\n",
      "   [0.57254905 0.65882355 0.71764708]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.47058824 0.49803922 0.52941179]\n",
      "   [0.38039216 0.45490196 0.61960787]\n",
      "   ...\n",
      "   [0.14117648 0.16078432 0.16862746]\n",
      "   [0.10196079 0.10980392 0.1254902 ]\n",
      "   [0.1254902  0.14117648 0.16078432]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.50196081 0.52156866 0.54509807]\n",
      "   [0.40784314 0.49803922 0.64705884]\n",
      "   ...\n",
      "   [0.1254902  0.14509805 0.16078432]\n",
      "   [0.13333334 0.15294118 0.16862746]\n",
      "   [0.13725491 0.15686275 0.17254902]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.35294119 0.3882353  0.44313726]\n",
      "   [0.41960785 0.50196081 0.63529414]\n",
      "   ...\n",
      "   [0.13333334 0.15294118 0.16862746]\n",
      "   [0.11764706 0.13725491 0.15294118]\n",
      "   [0.12156863 0.14117648 0.15686275]]]\n",
      "\n",
      "\n",
      " [[[0.20392157 0.25490198 0.28627452]\n",
      "   [0.28235295 0.35294119 0.39215687]\n",
      "   [0.26666668 0.33725491 0.38039216]\n",
      "   ...\n",
      "   [0.43921569 0.52156866 0.60392159]\n",
      "   [0.36470589 0.44705883 0.52941179]\n",
      "   [0.27450982 0.35686275 0.43921569]]\n",
      "\n",
      "  [[0.3764706  0.4627451  0.52549022]\n",
      "   [0.42745098 0.53725493 0.60392159]\n",
      "   [0.42745098 0.53333336 0.60000002]\n",
      "   ...\n",
      "   [0.36078432 0.4509804  0.52941179]\n",
      "   [0.41176471 0.49411765 0.57647061]\n",
      "   [0.32549021 0.40784314 0.49019608]]\n",
      "\n",
      "  [[0.30980393 0.36078432 0.40784314]\n",
      "   [0.28235295 0.33333334 0.36862746]\n",
      "   [0.30980393 0.35686275 0.39607844]\n",
      "   ...\n",
      "   [0.01176471 0.09411765 0.17647059]\n",
      "   [0.23137255 0.31764707 0.40392157]\n",
      "   [0.30588236 0.39215687 0.47450981]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.5411765  0.54901963 0.5529412 ]\n",
      "   [0.66666669 0.70980394 0.74901962]\n",
      "   ...\n",
      "   [0.36078432 0.39607844 0.42352942]\n",
      "   [0.31764707 0.34509805 0.3764706 ]\n",
      "   [0.35294119 0.3764706  0.40000001]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.56078434 0.56470591 0.57254905]\n",
      "   [0.72156864 0.7647059  0.81568629]\n",
      "   ...\n",
      "   [0.34509805 0.37254903 0.39607844]\n",
      "   [0.35686275 0.36862746 0.39215687]\n",
      "   [0.36078432 0.36862746 0.40000001]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.41568628 0.42745098 0.44705883]\n",
      "   [0.6156863  0.65098041 0.69803923]\n",
      "   ...\n",
      "   [0.35686275 0.36470589 0.38431373]\n",
      "   [0.34901962 0.34901962 0.36470589]\n",
      "   [0.34901962 0.35294119 0.37254903]]]].\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the variational autoencoder\n",
    "optimizer = optimizers.Adam(learning_rate=0.0005)\n",
    "vae.compile(optimizer=optimizer)\n",
    "\n",
    "# Create a model save checkpoint\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=\"./checkpoint.keras\",\n",
    "    save_weights_only=False,\n",
    "    save_freq=\"epoch\",\n",
    "    monitor=\"loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    "    verbose=0,\n",
    ")\n",
    "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "\n",
    "vae.fit(\n",
    "    train_generator_Mapa,\n",
    "    epochs=10,\n",
    "    batch_size=BATCH,\n",
    "    validation_data=(test_generator_Mapa, test_generator_Mapa),\n",
    "    callbacks=[model_checkpoint_callback, tensorboard_callback],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
